{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODXu3eohljRk4BoscNRpLR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IraDionisio/CSCI-166-DQN-Final-Project/blob/main/DoubleDQNBeamRider.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ct_vwFb06G4l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJHcqDf82zOI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test 3 --- Final Test for 06.11.2025"
      ],
      "metadata": {
        "id": "XtOuIKifmmyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[atari,accept-rom-license]\n",
        "!pip install autorom\n",
        "!pip install stable-baselines3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqL9333gml9l",
        "outputId": "1f917256-8e67-4bc1-ea8e-a9588a021ae8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "\u001b[33mWARNING: gymnasium 1.2.2 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Requirement already satisfied: ale_py>=0.9 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari]) (0.11.2)\n",
            "Collecting autorom\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from autorom) (8.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from autorom) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->autorom) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->autorom) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->autorom) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->autorom) (2025.11.12)\n",
            "Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Installing collected packages: autorom\n",
            "Successfully installed autorom-0.6.1\n",
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: gymnasium<1.3.0,>=0.29.1 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (1.2.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (2.0.2)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (2.9.0+cu126)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (3.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium<1.3.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.3)\n",
            "Downloading stable_baselines3-2.7.0-py3-none-any.whl (187 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: stable-baselines3\n",
            "Successfully installed stable-baselines3-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!AutoROM --accept-license"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi2qmVoVmxve",
        "outputId": "1e90091b-3a69-4a47-87cd-4a19a3022af8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoROM will download the Atari 2600 ROMs.\n",
            "They will be installed to:\n",
            "\t/usr/local/lib/python3.12/dist-packages/AutoROM/roms\n",
            "\n",
            "Existing ROMs will be overwritten.\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/adventure.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/air_raid.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/alien.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/amidar.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/assault.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/asterix.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/asteroids.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/atlantis.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/atlantis2.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/backgammon.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/bank_heist.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/basic_math.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/battle_zone.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/beam_rider.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/berzerk.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/blackjack.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/bowling.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/boxing.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/breakout.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/carnival.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/casino.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/centipede.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/chopper_command.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/combat.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/crazy_climber.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/crossbow.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/darkchambers.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/defender.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/demon_attack.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/donkey_kong.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/double_dunk.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/earthworld.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/elevator_action.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/enduro.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/entombed.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/et.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/fishing_derby.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/flag_capture.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/freeway.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/frogger.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/frostbite.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/galaxian.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/gopher.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/gravitar.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/hangman.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/haunted_house.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/hero.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/human_cannonball.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/ice_hockey.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/jamesbond.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/journey_escape.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/joust.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/kaboom.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/kangaroo.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/keystone_kapers.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/king_kong.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/klax.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/koolaid.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/krull.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/kung_fu_master.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/laser_gates.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/lost_luggage.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/mario_bros.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/maze_craze.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/miniature_golf.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/montezuma_revenge.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/mr_do.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/ms_pacman.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/name_this_game.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/othello.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/pacman.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/phoenix.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/pitfall.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/pitfall2.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/pong.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/pooyan.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/private_eye.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/qbert.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/riverraid.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/road_runner.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/robotank.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/seaquest.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/sir_lancelot.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/skiing.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/solaris.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/space_invaders.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/space_war.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/star_gunner.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/superman.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/surround.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/tennis.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/tetris.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/tic_tac_toe_3d.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/time_pilot.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/trondead.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/turmoil.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/tutankham.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/up_n_down.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/venture.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/video_checkers.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/video_chess.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/video_cube.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/video_pinball.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/warlords.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/wizard_of_wor.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/word_zapper.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/yars_revenge.bin\n",
            "Installed /usr/local/lib/python3.12/dist-packages/AutoROM/roms/zaxxon.bin\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install the Gym"
      ],
      "metadata": {
        "id": "RQyqgMFzT-qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ale_py\n",
        "import gymnasium as gym"
      ],
      "metadata": {
        "id": "_W_afhrzUAnp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configure the model save drive"
      ],
      "metadata": {
        "id": "E0n7zQrALq7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "v8hSrWrrLibc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22654eda-31da-417d-86f2-b08d2f3794e6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "save_dir = \"/content/drive/MyDrive/PUBLIC/Models\"\n",
        "os.makedirs(save_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "vlWPUjKfLv9y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now Model"
      ],
      "metadata": {
        "id": "SAEvyoukL1T3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "import argparse\n",
        "import time\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import collections\n",
        "import typing as tt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.tensorboard.writer import SummaryWriter"
      ],
      "metadata": {
        "id": "VCbjkLLSxCUN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dqn_model\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        size = self.conv(torch.zeros(1, *input_shape)).size()[-1]\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "    def forward(self, x: torch.ByteTensor):\n",
        "        x = x.float() / 255.0\n",
        "        return self.fc(self.conv(x))"
      ],
      "metadata": {
        "id": "dIJ32Rs6xJsV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wrappers\n",
        "\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3.common import atari_wrappers\n",
        "\n",
        "\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    ImageToPyTorch: Reorders image dimensions from (H, W, C) to (C, H, W)\n",
        "    for compatibility with PyTorch convolutional layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        super(ImageToPyTorch, self).__init__(env)\n",
        "        obs = self.observation_space\n",
        "        assert isinstance(obs, gym.spaces.Box)\n",
        "        assert len(obs.shape) == 3\n",
        "        new_shape = (obs.shape[-1], obs.shape[0], obs.shape[1])\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=obs.low.min(), high=obs.high.max(),\n",
        "            shape=new_shape, dtype=obs.dtype)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "    BufferWrapper: Maintains a rolling window of the last `n_steps` frames\n",
        "    to give the agent a sense of temporal context.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, n_steps):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        obs = env.observation_space\n",
        "        assert isinstance(obs, spaces.Box)\n",
        "        new_obs = gym.spaces.Box(\n",
        "            obs.low.repeat(n_steps, axis=0), obs.high.repeat(n_steps, axis=0),\n",
        "            dtype=obs.dtype)\n",
        "        self.observation_space = new_obs\n",
        "        self.buffer = collections.deque(maxlen=n_steps)\n",
        "\n",
        "    def reset(self, *, seed: tt.Optional[int] = None, options: tt.Optional[dict[str, tt.Any]] = None):\n",
        "        for _ in range(self.buffer.maxlen):\n",
        "            self.buffer.append(np.zeros_like(self.env.observation_space.low))\n",
        "        obs, extra = self.env.reset()\n",
        "        return self.observation(obs), extra\n",
        "\n",
        "    def observation(self, observation: np.ndarray) -> np.ndarray:\n",
        "        self.buffer.append(observation)\n",
        "        return np.concatenate(self.buffer)\n",
        "\n",
        "\n",
        "def make_env(env_name: str, render_mode=None, **kwargs):\n",
        "    print(f\"Creating environment {env_name}\")\n",
        "    env = gym.make(env_name, render_mode=render_mode, **kwargs)\n",
        "    env = atari_wrappers.AtariWrapper(env, clip_reward=False, noop_max=0)\n",
        "    env = ImageToPyTorch(env)\n",
        "    env = BufferWrapper(env, n_steps=N_STEPS)\n",
        "    return env"
      ],
      "metadata": {
        "id": "Vk2CtGcOBcQo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d7be06-b123-4697-8bac-b415e00da60c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_ENV_NAME = \"ALE/BeamRider-v5\"\n",
        "MEAN_REWARD_BOUND = 500 # Default value, will be overridden by fast training config\n",
        "\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_SIZE = 10000\n",
        "LEARNING_RATE = 1e-4\n",
        "SYNC_TARGET_FRAMES = 1000\n",
        "REPLAY_START_SIZE = 10000\n",
        "N_STEPS = 4 # for frame-stacking\n",
        "\n",
        "SAVE_EPSILON = 0.5  # Only save if at least this much better\n",
        "EPSILON_DECAY_LAST_FRAME = 150000\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_FINAL = 0.01\n",
        "\n",
        "# Tuple of tensors returned from a sampled minibatch in replay buffer\n",
        "State = np.ndarray\n",
        "Action = int\n",
        "BatchTensors = tt.Tuple[\n",
        "    torch.ByteTensor,           # current state\n",
        "    torch.LongTensor,           # actions\n",
        "    torch.Tensor,               # rewards\n",
        "    torch.BoolTensor,           # done || trunc\n",
        "    torch.ByteTensor            # next state\n",
        "]"
      ],
      "metadata": {
        "id": "GvXPdjPCBxOd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âš™ï¸ Fast Training Config for Quick Test Run\n",
        "MEAN_REWARD_BOUND = 500 # Changed to allow for more meaningful training\n",
        "REPLAY_START_SIZE = 1000\n",
        "EPSILON_DECAY_LAST_FRAME = 10_000\n",
        "SYNC_TARGET_FRAMES = 500\n",
        "\n",
        "# REPLAY_SIZE = 5000  # optional\n",
        "# BATCH_SIZE = 16     # optional"
      ],
      "metadata": {
        "id": "3FHvHNMrR9Sk"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Define directories\n",
        "save_dir_drive = \"/content/drive/MyDrive/PUBLIC/Models\"\n",
        "save_dir_local = \"saved_models\"\n",
        "\n",
        "# Create both directories if they don't exist\n",
        "os.makedirs(save_dir_drive, exist_ok=True)\n",
        "os.makedirs(save_dir_local, exist_ok=True)\n",
        "\n",
        "# Safe model filename\n",
        "env_name = DEFAULT_ENV_NAME\n",
        "safe_env_name = env_name.replace(\"/\", \"_\")"
      ],
      "metadata": {
        "id": "MkILCuT2OhBV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Experience:\n",
        "    state: State\n",
        "    action: Action\n",
        "    reward: float\n",
        "    done_trunc: bool\n",
        "    new_state: State\n",
        "\n",
        "\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity: int):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience: Experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size: int) -> tt.List[Experience]:\n",
        "        indices = np.random.choice(len(self), batch_size, replace=False)\n",
        "        return [self.buffer[idx] for idx in indices]"
      ],
      "metadata": {
        "id": "uW4Bo-mcB6rc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent:\n",
        "    def __init__(self, env: gym.Env, exp_buffer: ExperienceBuffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self.state: tt.Optional[np.ndarray] = None\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state, _ = self.env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def play_step(self, net: DQN, device: torch.device,\n",
        "                  epsilon: float = 0.0) -> tt.Optional[float]:\n",
        "        done_reward = None\n",
        "\n",
        "        if np.random.random() < epsilon:\n",
        "            action = env.action_space.sample()\n",
        "        else:\n",
        "            state_v = torch.as_tensor(self.state).to(device)\n",
        "            state_v.unsqueeze_(0)\n",
        "            q_vals_v = net(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "\n",
        "        # do step in the environment\n",
        "        new_state, reward, is_done, is_tr, _ = self.env.step(action)\n",
        "        self.total_reward += reward\n",
        "\n",
        "        exp = Experience(\n",
        "            state=self.state, action=action, reward=float(reward),\n",
        "            done_trunc=is_done or is_tr, new_state=new_state\n",
        "        )\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done or is_tr:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward"
      ],
      "metadata": {
        "id": "irJb4V32B-R8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_to_tensors(batch: tt.List[Experience], device: torch.device) -> BatchTensors:\n",
        "    states, actions, rewards, dones, new_state = [], [], [], [], []\n",
        "    for e in batch:\n",
        "        states.append(e.state)\n",
        "        actions.append(e.action)\n",
        "        rewards.append(e.reward)\n",
        "        dones.append(e.done_trunc)\n",
        "        new_state.append(e.new_state)\n",
        "    states_t = torch.as_tensor(np.asarray(states))\n",
        "    actions_t = torch.LongTensor(actions)\n",
        "    rewards_t = torch.FloatTensor(rewards)\n",
        "    dones_t = torch.BoolTensor(dones)\n",
        "    new_states_t = torch.as_tensor(np.asarray(new_state))\n",
        "    return states_t.to(device), actions_t.to(device), rewards_t.to(device), \\\n",
        "           dones_t.to(device),  new_states_t.to(device)"
      ],
      "metadata": {
        "id": "vHXmNr_wCBJm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss(batch: tt.List[Experience], net: DQN, tgt_net: DQN,\n",
        "              device: torch.device) -> torch.Tensor:\n",
        "    states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch, device)\n",
        "\n",
        "    state_action_values = net(states_t).gather(\n",
        "        1, actions_t.unsqueeze(-1)\n",
        "    ).squeeze(-1)\n",
        "    with torch.no_grad():\n",
        "        # Double DQN change: Use online net to select action, target net to evaluate Q-value\n",
        "        next_state_actions = net(new_states_t).argmax(dim=1)\n",
        "        next_state_values = tgt_net(new_states_t).gather(\n",
        "            1, next_state_actions.unsqueeze(-1)\n",
        "        ).squeeze(-1)\n",
        "        next_state_values[dones_t] = 0.0\n",
        "        next_state_values = next_state_values.detach()\n",
        "\n",
        "    expected_state_action_values = next_state_values * GAMMA + rewards_t\n",
        "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
      ],
      "metadata": {
        "id": "-dbh0431CEXO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_comment = f\"lr{LEARNING_RATE}_gamma{GAMMA}_epsdec{EPSILON_DECAY_LAST_FRAME}_rs{REPLAY_START_SIZE}_bs{BATCH_SIZE}_sync{SYNC_TARGET_FRAMES}_fs{N_STEPS}\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "env = make_env(env_name)\n",
        "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "writer = SummaryWriter(comment=f\"-{env_name}-{model_comment}\")\n",
        "print(net)\n",
        "\n",
        "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "agent = Agent(env, buffer)\n",
        "epsilon = EPSILON_START\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "total_rewards = []\n",
        "frame_idx = 0\n",
        "ts_frame = 0\n",
        "ts = time.time()\n",
        "best_m_reward = None\n",
        "\n",
        "hparams = {\n",
        "    'learning_rate': LEARNING_RATE,\n",
        "    'gamma': GAMMA,\n",
        "    'epsilon_start': EPSILON_START,\n",
        "    'epsilon_final': EPSILON_FINAL,\n",
        "    'epsilon_decay_last_frame': EPSILON_DECAY_LAST_FRAME,\n",
        "    'replay_size': REPLAY_SIZE,\n",
        "    'replay_start_size': REPLAY_START_SIZE,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'sync_target_frames': SYNC_TARGET_FRAMES,\n",
        "    'frame_stack': N_STEPS,\n",
        "    'optimizer': 'Adam',\n",
        "    'mean_reward_bound': MEAN_REWARD_BOUND\n",
        "}\n",
        "\n",
        "start_time = time.time()\n",
        "while True:\n",
        "    frame_idx += 1\n",
        "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "    reward = agent.play_step(net, device, epsilon)\n",
        "    if reward is not None:\n",
        "        total_rewards.append(reward)\n",
        "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "        elapsed = time.time() - start_time  # in seconds\n",
        "        ts_frame = frame_idx\n",
        "        ts = time.time()\n",
        "        m_reward = np.mean(total_rewards[-100:])\n",
        "        print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "             f\"eps {epsilon:.2f}, speed {speed:.2f} f/s, time {elapsed/60:.1f} min\")\n",
        "        writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "        writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "        writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
        "        writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "        if best_m_reward is None or m_reward > best_m_reward + SAVE_EPSILON:\n",
        "            # print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "            #    f\"eps {epsilon:.2f}, speed {speed:.2f} f/s, time {elapsed/60:.1f} min\")\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "            model_filename = f\"{safe_env_name}-best_{int(m_reward)}-{timestamp}-{model_comment}.dat\"\n",
        "\n",
        "            # Save to both paths\n",
        "            model_path_drive = os.path.join(save_dir_drive, model_filename)\n",
        "            model_path_local = os.path.join(save_dir_local, model_filename)\n",
        "\n",
        "            torch.save(net.state_dict(), model_path_drive)\n",
        "            torch.save(net.state_dict(), model_path_local)\n",
        "\n",
        "            print(f\"ðŸ’¾ Model saved to:\\n - Google Drive: {model_path_drive}\\n - Local:        {model_path_local}\")\n",
        "            if best_m_reward is not None:\n",
        "                print(f\"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}\")\n",
        "            best_m_reward = m_reward\n",
        "\n",
        "            writer.add_hparams(hparams, {'metric/mean_reward': m_reward}, global_step=frame_idx)\n",
        "\n",
        "        if m_reward > MEAN_REWARD_BOUND:\n",
        "            print(\"Solved in %d frames!\" % frame_idx)\n",
        "            break\n",
        "    if len(buffer) < REPLAY_START_SIZE:\n",
        "        continue\n",
        "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "        tgt_net.load_state_dict(net.state_dict())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    batch = buffer.sample(BATCH_SIZE)\n",
        "    loss_t = calc_loss(batch, net, tgt_net, device)\n",
        "    loss_t.backward()\n",
        "    optimizer.step()\n",
        "env.close()\n",
        "writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q8gj-5woCXEB",
        "outputId": "dd525748-775f-4f30-8c21-8a4d650fa8b6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating environment ALE/BeamRider-v5\n",
            "DQN(\n",
            "  (conv): Sequential(\n",
            "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
            "    (3): ReLU()\n",
            "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (5): ReLU()\n",
            "    (6): Flatten(start_dim=1, end_dim=-1)\n",
            "  )\n",
            "  (fc): Sequential(\n",
            "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=9, bias=True)\n",
            "  )\n",
            ")\n",
            "156: done 1 games, reward 132.000, eps 0.98, speed 202.90 f/s, time 0.0 min\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_BeamRider-v5-best_132-20251201-0319-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4.dat\n",
            " - Local:        saved_models/ALE_BeamRider-v5-best_132-20251201-0319-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4.dat\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "240: done 2 games, reward 110.000, eps 0.98, speed 158.14 f/s, time 0.0 min\n",
            "291: done 3 games, reward 88.000, eps 0.97, speed 164.29 f/s, time 0.0 min\n",
            "401: done 4 games, reward 165.000, eps 0.96, speed 181.54 f/s, time 0.0 min\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_BeamRider-v5-best_165-20251201-0319-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4.dat\n",
            " - Local:        saved_models/ALE_BeamRider-v5-best_165-20251201-0319-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4.dat\n",
            "Best reward updated 132.000 -> 165.000\n",
            "529: done 5 games, reward 132.000, eps 0.95, speed 165.58 f/s, time 0.0 min\n",
            "787: done 6 games, reward 154.000, eps 0.92, speed 210.63 f/s, time 0.1 min\n",
            "881: done 7 games, reward 150.857, eps 0.91, speed 265.72 f/s, time 0.1 min\n",
            "985: done 8 games, reward 143.000, eps 0.90, speed 280.35 f/s, time 0.1 min\n",
            "1151: done 9 games, reward 132.000, eps 0.88, speed 11.00 f/s, time 0.3 min\n",
            "1306: done 10 games, reward 132.000, eps 0.87, speed 8.62 f/s, time 0.6 min\n",
            "1391: done 11 games, reward 124.000, eps 0.86, speed 9.80 f/s, time 0.8 min\n",
            "1442: done 12 games, reward 113.667, eps 0.86, speed 11.91 f/s, time 0.8 min\n",
            "1576: done 13 games, reward 128.615, eps 0.84, speed 10.61 f/s, time 1.1 min\n",
            "1635: done 14 games, reward 119.429, eps 0.84, speed 11.11 f/s, time 1.1 min\n",
            "1693: done 15 games, reward 117.333, eps 0.83, speed 8.99 f/s, time 1.3 min\n",
            "1813: done 16 games, reward 118.250, eps 0.82, speed 9.08 f/s, time 1.5 min\n",
            "1870: done 17 games, reward 116.471, eps 0.81, speed 9.49 f/s, time 1.6 min\n",
            "1958: done 18 games, reward 117.333, eps 0.80, speed 8.64 f/s, time 1.7 min\n",
            "2079: done 19 games, reward 118.105, eps 0.79, speed 8.77 f/s, time 2.0 min\n",
            "2141: done 20 games, reward 114.400, eps 0.79, speed 8.65 f/s, time 2.1 min\n",
            "2212: done 21 games, reward 111.048, eps 0.78, speed 9.72 f/s, time 2.2 min\n",
            "2384: done 22 games, reward 114.000, eps 0.76, speed 9.57 f/s, time 2.5 min\n",
            "2516: done 23 games, reward 110.957, eps 0.75, speed 8.88 f/s, time 2.8 min\n",
            "2592: done 24 games, reward 110.000, eps 0.74, speed 8.67 f/s, time 2.9 min\n",
            "2661: done 25 games, reward 110.880, eps 0.73, speed 8.16 f/s, time 3.1 min\n",
            "2806: done 26 games, reward 118.462, eps 0.72, speed 9.45 f/s, time 3.3 min\n",
            "2853: done 27 games, reward 115.704, eps 0.71, speed 10.21 f/s, time 3.4 min\n",
            "2975: done 28 games, reward 114.714, eps 0.70, speed 9.46 f/s, time 3.6 min\n",
            "3140: done 29 games, reward 115.310, eps 0.69, speed 8.41 f/s, time 3.9 min\n",
            "3194: done 30 games, reward 111.467, eps 0.68, speed 9.04 f/s, time 4.0 min\n",
            "3245: done 31 games, reward 109.290, eps 0.68, speed 8.56 f/s, time 4.1 min\n",
            "3618: done 32 games, reward 115.500, eps 0.64, speed 8.88 f/s, time 4.8 min\n",
            "3665: done 33 games, reward 113.333, eps 0.63, speed 9.53 f/s, time 4.9 min\n",
            "3729: done 34 games, reward 110.000, eps 0.63, speed 9.59 f/s, time 5.0 min\n",
            "3867: done 35 games, reward 113.143, eps 0.61, speed 9.00 f/s, time 5.3 min\n",
            "3924: done 36 games, reward 111.222, eps 0.61, speed 8.40 f/s, time 5.4 min\n",
            "4116: done 37 games, reward 115.351, eps 0.59, speed 8.99 f/s, time 5.7 min\n",
            "4237: done 38 games, reward 118.105, eps 0.58, speed 9.46 f/s, time 6.0 min\n",
            "4302: done 39 games, reward 118.462, eps 0.57, speed 9.74 f/s, time 6.1 min\n",
            "4350: done 40 games, reward 116.600, eps 0.56, speed 9.97 f/s, time 6.1 min\n",
            "4464: done 41 games, reward 115.902, eps 0.55, speed 9.32 f/s, time 6.4 min\n",
            "4559: done 42 games, reward 115.238, eps 0.54, speed 9.02 f/s, time 6.5 min\n",
            "4725: done 43 games, reward 120.744, eps 0.53, speed 9.50 f/s, time 6.8 min\n",
            "4907: done 44 games, reward 120.000, eps 0.51, speed 8.95 f/s, time 7.2 min\n",
            "5056: done 45 games, reward 123.200, eps 0.49, speed 9.44 f/s, time 7.4 min\n",
            "5164: done 46 games, reward 123.391, eps 0.48, speed 9.00 f/s, time 7.6 min\n",
            "5245: done 47 games, reward 124.511, eps 0.48, speed 9.28 f/s, time 7.8 min\n",
            "5296: done 48 games, reward 122.833, eps 0.47, speed 9.00 f/s, time 7.9 min\n",
            "5496: done 49 games, reward 126.612, eps 0.45, speed 9.17 f/s, time 8.2 min\n",
            "5627: done 50 games, reward 131.120, eps 0.44, speed 9.16 f/s, time 8.5 min\n",
            "5675: done 51 games, reward 130.275, eps 0.43, speed 9.56 f/s, time 8.5 min\n",
            "5736: done 52 games, reward 127.769, eps 0.43, speed 9.91 f/s, time 8.6 min\n",
            "5851: done 53 games, reward 129.509, eps 0.41, speed 9.22 f/s, time 8.9 min\n",
            "5898: done 54 games, reward 127.111, eps 0.41, speed 8.11 f/s, time 9.0 min\n",
            "5959: done 55 games, reward 127.200, eps 0.40, speed 10.14 f/s, time 9.1 min\n",
            "6059: done 56 games, reward 127.286, eps 0.39, speed 9.06 f/s, time 9.2 min\n",
            "6138: done 57 games, reward 127.368, eps 0.39, speed 8.90 f/s, time 9.4 min\n",
            "6193: done 58 games, reward 125.172, eps 0.38, speed 10.10 f/s, time 9.5 min\n",
            "6368: done 59 games, reward 130.508, eps 0.36, speed 9.36 f/s, time 9.8 min\n",
            "6486: done 60 games, reward 130.533, eps 0.35, speed 9.26 f/s, time 10.0 min\n",
            "6578: done 61 games, reward 129.115, eps 0.34, speed 8.94 f/s, time 10.2 min\n",
            "6676: done 62 games, reward 129.161, eps 0.33, speed 8.94 f/s, time 10.4 min\n",
            "6734: done 63 games, reward 127.111, eps 0.33, speed 8.04 f/s, time 10.5 min\n",
            "6798: done 64 games, reward 127.875, eps 0.32, speed 8.76 f/s, time 10.6 min\n",
            "7101: done 65 games, reward 133.354, eps 0.29, speed 9.28 f/s, time 11.1 min\n",
            "7172: done 66 games, reward 132.667, eps 0.28, speed 8.70 f/s, time 11.3 min\n",
            "7220: done 67 games, reward 131.343, eps 0.28, speed 10.00 f/s, time 11.4 min\n",
            "7371: done 68 games, reward 130.706, eps 0.26, speed 8.94 f/s, time 11.6 min\n",
            "7466: done 69 games, reward 130.725, eps 0.25, speed 9.66 f/s, time 11.8 min\n",
            "7561: done 70 games, reward 130.114, eps 0.24, speed 8.89 f/s, time 12.0 min\n",
            "7847: done 71 games, reward 136.338, eps 0.22, speed 9.28 f/s, time 12.5 min\n",
            "7891: done 72 games, reward 134.444, eps 0.21, speed 7.97 f/s, time 12.6 min\n",
            "8062: done 73 games, reward 133.808, eps 0.19, speed 9.31 f/s, time 12.9 min\n",
            "8222: done 74 games, reward 133.189, eps 0.18, speed 9.22 f/s, time 13.2 min\n",
            "8442: done 75 games, reward 133.760, eps 0.16, speed 9.06 f/s, time 13.6 min\n",
            "8584: done 76 games, reward 134.316, eps 0.14, speed 9.20 f/s, time 13.8 min\n",
            "8685: done 77 games, reward 137.143, eps 0.13, speed 8.94 f/s, time 14.0 min\n",
            "8840: done 78 games, reward 138.205, eps 0.12, speed 9.27 f/s, time 14.3 min\n",
            "8977: done 79 games, reward 137.570, eps 0.10, speed 9.19 f/s, time 14.6 min\n",
            "9152: done 80 games, reward 139.700, eps 0.08, speed 8.83 f/s, time 14.9 min\n",
            "9204: done 81 games, reward 138.519, eps 0.08, speed 9.95 f/s, time 15.0 min\n",
            "9349: done 82 games, reward 138.976, eps 0.07, speed 9.29 f/s, time 15.2 min\n",
            "9496: done 83 games, reward 139.952, eps 0.05, speed 8.10 f/s, time 15.5 min\n",
            "9557: done 84 games, reward 138.810, eps 0.04, speed 10.09 f/s, time 15.6 min\n",
            "9621: done 85 games, reward 137.694, eps 0.04, speed 8.47 f/s, time 15.8 min\n",
            "9781: done 86 games, reward 140.698, eps 0.02, speed 9.27 f/s, time 16.1 min\n",
            "9829: done 87 games, reward 139.080, eps 0.02, speed 9.96 f/s, time 16.1 min\n",
            "9980: done 88 games, reward 139.000, eps 0.01, speed 8.97 f/s, time 16.4 min\n",
            "10165: done 89 games, reward 139.910, eps 0.01, speed 9.19 f/s, time 16.7 min\n",
            "10220: done 90 games, reward 139.333, eps 0.01, speed 9.90 f/s, time 16.8 min\n",
            "10369: done 91 games, reward 139.253, eps 0.01, speed 8.81 f/s, time 17.1 min\n",
            "10450: done 92 games, reward 140.130, eps 0.01, speed 10.00 f/s, time 17.3 min\n",
            "10568: done 93 games, reward 140.043, eps 0.01, speed 9.16 f/s, time 17.5 min\n",
            "10629: done 94 games, reward 139.957, eps 0.01, speed 8.55 f/s, time 17.6 min\n",
            "10732: done 95 games, reward 140.337, eps 0.01, speed 9.78 f/s, time 17.8 min\n",
            "10805: done 96 games, reward 139.333, eps 0.01, speed 9.09 f/s, time 17.9 min\n",
            "10959: done 97 games, reward 138.804, eps 0.01, speed 9.47 f/s, time 18.2 min\n",
            "11093: done 98 games, reward 138.735, eps 0.01, speed 9.35 f/s, time 18.4 min\n",
            "11145: done 99 games, reward 137.333, eps 0.01, speed 8.20 f/s, time 18.5 min\n",
            "11304: done 100 games, reward 137.720, eps 0.01, speed 9.48 f/s, time 18.8 min\n",
            "11395: done 101 games, reward 137.720, eps 0.01, speed 8.98 f/s, time 19.0 min\n",
            "11483: done 102 games, reward 138.160, eps 0.01, speed 10.06 f/s, time 19.1 min\n",
            "11530: done 103 games, reward 137.720, eps 0.01, speed 8.11 f/s, time 19.2 min\n",
            "11645: done 104 games, reward 135.080, eps 0.01, speed 9.16 f/s, time 19.4 min\n",
            "11715: done 105 games, reward 135.520, eps 0.01, speed 10.04 f/s, time 19.5 min\n",
            "11817: done 106 games, reward 134.200, eps 0.01, speed 9.06 f/s, time 19.7 min\n",
            "11914: done 107 games, reward 135.080, eps 0.01, speed 8.94 f/s, time 19.9 min\n",
            "11978: done 108 games, reward 134.200, eps 0.01, speed 10.07 f/s, time 20.0 min\n",
            "12207: done 109 games, reward 136.400, eps 0.01, speed 9.18 f/s, time 20.4 min\n",
            "12425: done 110 games, reward 137.720, eps 0.01, speed 8.64 f/s, time 20.8 min\n",
            "12540: done 111 games, reward 138.600, eps 0.01, speed 9.15 f/s, time 21.1 min\n",
            "12587: done 112 games, reward 138.600, eps 0.01, speed 10.13 f/s, time 21.1 min\n",
            "12715: done 113 games, reward 136.840, eps 0.01, speed 9.31 f/s, time 21.4 min\n",
            "12874: done 114 games, reward 139.920, eps 0.01, speed 9.49 f/s, time 21.6 min\n",
            "12925: done 115 games, reward 139.480, eps 0.01, speed 8.15 f/s, time 21.7 min\n",
            "13127: done 116 games, reward 141.240, eps 0.01, speed 9.60 f/s, time 22.1 min\n",
            "13215: done 117 games, reward 141.240, eps 0.01, speed 8.87 f/s, time 22.3 min\n",
            "13277: done 118 games, reward 140.800, eps 0.01, speed 9.38 f/s, time 22.4 min\n",
            "13381: done 119 games, reward 140.800, eps 0.01, speed 9.47 f/s, time 22.6 min\n",
            "13465: done 120 games, reward 141.240, eps 0.01, speed 8.87 f/s, time 22.7 min\n",
            "13556: done 121 games, reward 142.120, eps 0.01, speed 8.99 f/s, time 22.9 min\n",
            "13741: done 122 games, reward 143.880, eps 0.01, speed 9.47 f/s, time 23.2 min\n",
            "13886: done 123 games, reward 145.640, eps 0.01, speed 9.37 f/s, time 23.5 min\n",
            "13967: done 124 games, reward 145.640, eps 0.01, speed 8.80 f/s, time 23.6 min\n",
            "14061: done 125 games, reward 146.080, eps 0.01, speed 8.95 f/s, time 23.8 min\n",
            "14203: done 126 games, reward 146.080, eps 0.01, speed 9.27 f/s, time 24.0 min\n",
            "14316: done 127 games, reward 147.400, eps 0.01, speed 9.16 f/s, time 24.3 min\n",
            "14450: done 128 games, reward 149.600, eps 0.01, speed 9.36 f/s, time 24.5 min\n",
            "14555: done 129 games, reward 149.600, eps 0.01, speed 9.09 f/s, time 24.7 min\n",
            "14610: done 130 games, reward 150.480, eps 0.01, speed 10.19 f/s, time 24.8 min\n",
            "14708: done 131 games, reward 151.360, eps 0.01, speed 8.99 f/s, time 25.0 min\n",
            "14861: done 132 games, reward 150.480, eps 0.01, speed 9.44 f/s, time 25.2 min\n",
            "14909: done 133 games, reward 150.480, eps 0.01, speed 9.62 f/s, time 25.3 min\n",
            "15076: done 134 games, reward 152.680, eps 0.01, speed 8.51 f/s, time 25.6 min\n",
            "15131: done 135 games, reward 151.360, eps 0.01, speed 10.17 f/s, time 25.7 min\n",
            "15189: done 136 games, reward 151.360, eps 0.01, speed 8.71 f/s, time 25.8 min\n",
            "15472: done 137 games, reward 154.440, eps 0.01, speed 9.36 f/s, time 26.3 min\n",
            "15627: done 138 games, reward 153.120, eps 0.01, speed 9.49 f/s, time 26.6 min\n",
            "15725: done 139 games, reward 152.760, eps 0.01, speed 9.08 f/s, time 26.8 min\n",
            "15835: done 140 games, reward 154.520, eps 0.01, speed 9.20 f/s, time 27.0 min\n",
            "15899: done 141 games, reward 154.080, eps 0.01, speed 10.18 f/s, time 27.1 min\n",
            "15953: done 142 games, reward 153.200, eps 0.01, speed 8.38 f/s, time 27.2 min\n",
            "16041: done 143 games, reward 151.000, eps 0.01, speed 10.20 f/s, time 27.3 min\n",
            "16105: done 144 games, reward 150.560, eps 0.01, speed 8.64 f/s, time 27.5 min\n",
            "16184: done 145 games, reward 148.800, eps 0.01, speed 10.18 f/s, time 27.6 min\n",
            "16423: done 146 games, reward 149.240, eps 0.01, speed 9.26 f/s, time 28.0 min\n",
            "16538: done 147 games, reward 147.920, eps 0.01, speed 9.19 f/s, time 28.2 min\n",
            "16592: done 148 games, reward 147.480, eps 0.01, speed 8.33 f/s, time 28.3 min\n",
            "16689: done 149 games, reward 146.160, eps 0.01, speed 10.02 f/s, time 28.5 min\n",
            "16775: done 150 games, reward 144.840, eps 0.01, speed 9.05 f/s, time 28.7 min\n",
            "16880: done 151 games, reward 145.720, eps 0.01, speed 9.10 f/s, time 28.9 min\n",
            "17254: done 152 games, reward 151.000, eps 0.01, speed 9.28 f/s, time 29.5 min\n",
            "17325: done 153 games, reward 150.120, eps 0.01, speed 9.57 f/s, time 29.7 min\n",
            "17364: done 154 games, reward 150.120, eps 0.01, speed 8.49 f/s, time 29.7 min\n",
            "17464: done 155 games, reward 149.680, eps 0.01, speed 9.26 f/s, time 29.9 min\n",
            "17524: done 156 games, reward 149.240, eps 0.01, speed 9.82 f/s, time 30.0 min\n",
            "17572: done 157 games, reward 147.920, eps 0.01, speed 10.00 f/s, time 30.1 min\n",
            "17790: done 158 games, reward 153.640, eps 0.01, speed 9.20 f/s, time 30.5 min\n",
            "17869: done 159 games, reward 149.680, eps 0.01, speed 7.84 f/s, time 30.7 min\n",
            "18014: done 160 games, reward 148.800, eps 0.01, speed 9.50 f/s, time 30.9 min\n",
            "18182: done 161 games, reward 151.440, eps 0.01, speed 9.55 f/s, time 31.2 min\n",
            "18297: done 162 games, reward 151.440, eps 0.01, speed 9.32 f/s, time 31.4 min\n",
            "18362: done 163 games, reward 151.440, eps 0.01, speed 8.64 f/s, time 31.5 min\n",
            "18506: done 164 games, reward 153.640, eps 0.01, speed 9.52 f/s, time 31.8 min\n",
            "18606: done 165 games, reward 150.560, eps 0.01, speed 9.17 f/s, time 32.0 min\n",
            "18664: done 166 games, reward 150.120, eps 0.01, speed 10.27 f/s, time 32.1 min\n",
            "18805: done 167 games, reward 151.000, eps 0.01, speed 9.46 f/s, time 32.3 min\n",
            "18936: done 168 games, reward 152.320, eps 0.01, speed 9.44 f/s, time 32.5 min\n",
            "19001: done 169 games, reward 151.000, eps 0.01, speed 8.66 f/s, time 32.7 min\n",
            "19119: done 170 games, reward 150.560, eps 0.01, speed 9.34 f/s, time 32.9 min\n",
            "19184: done 171 games, reward 145.280, eps 0.01, speed 10.24 f/s, time 33.0 min\n",
            "19252: done 172 games, reward 147.040, eps 0.01, speed 8.80 f/s, time 33.1 min\n",
            "19399: done 173 games, reward 147.920, eps 0.01, speed 9.49 f/s, time 33.4 min\n",
            "19497: done 174 games, reward 147.920, eps 0.01, speed 9.08 f/s, time 33.5 min\n",
            "19585: done 175 games, reward 147.920, eps 0.01, speed 10.31 f/s, time 33.7 min\n",
            "19735: done 176 games, reward 147.480, eps 0.01, speed 9.35 f/s, time 34.0 min\n",
            "19923: done 177 games, reward 148.800, eps 0.01, speed 9.29 f/s, time 34.3 min\n",
            "19992: done 178 games, reward 147.040, eps 0.01, speed 9.70 f/s, time 34.4 min\n",
            "20211: done 179 games, reward 148.360, eps 0.01, speed 9.44 f/s, time 34.8 min\n",
            "20258: done 180 games, reward 145.720, eps 0.01, speed 8.51 f/s, time 34.9 min\n",
            "20316: done 181 games, reward 145.280, eps 0.01, speed 9.82 f/s, time 35.0 min\n",
            "20487: done 182 games, reward 148.360, eps 0.01, speed 9.65 f/s, time 35.3 min\n",
            "20571: done 183 games, reward 147.480, eps 0.01, speed 9.04 f/s, time 35.4 min\n",
            "20798: done 184 games, reward 147.040, eps 0.01, speed 8.90 f/s, time 35.9 min\n",
            "20962: done 185 games, reward 149.240, eps 0.01, speed 9.60 f/s, time 36.2 min\n",
            "21134: done 186 games, reward 146.600, eps 0.01, speed 9.33 f/s, time 36.5 min\n",
            "21185: done 187 games, reward 146.600, eps 0.01, speed 9.25 f/s, time 36.6 min\n",
            "21298: done 188 games, reward 147.040, eps 0.01, speed 9.35 f/s, time 36.8 min\n",
            "21507: done 189 games, reward 147.920, eps 0.01, speed 9.68 f/s, time 37.1 min\n",
            "21559: done 190 games, reward 147.040, eps 0.01, speed 8.58 f/s, time 37.2 min\n",
            "21641: done 191 games, reward 147.040, eps 0.01, speed 9.85 f/s, time 37.4 min\n",
            "21824: done 192 games, reward 148.800, eps 0.01, speed 9.28 f/s, time 37.7 min\n",
            "21929: done 193 games, reward 147.920, eps 0.01, speed 9.24 f/s, time 37.9 min\n",
            "22100: done 194 games, reward 147.920, eps 0.01, speed 9.60 f/s, time 38.2 min\n",
            "22196: done 195 games, reward 147.920, eps 0.01, speed 9.17 f/s, time 38.3 min\n",
            "22260: done 196 games, reward 147.480, eps 0.01, speed 10.36 f/s, time 38.4 min\n",
            "22377: done 197 games, reward 149.680, eps 0.01, speed 9.28 f/s, time 38.7 min\n",
            "22434: done 198 games, reward 148.800, eps 0.01, speed 8.49 f/s, time 38.8 min\n",
            "22538: done 199 games, reward 150.120, eps 0.01, speed 9.78 f/s, time 38.9 min\n",
            "22706: done 200 games, reward 150.560, eps 0.01, speed 9.27 f/s, time 39.2 min\n",
            "22793: done 201 games, reward 150.120, eps 0.01, speed 9.83 f/s, time 39.4 min\n",
            "22851: done 202 games, reward 148.800, eps 0.01, speed 9.14 f/s, time 39.5 min\n",
            "22991: done 203 games, reward 152.320, eps 0.01, speed 9.49 f/s, time 39.7 min\n",
            "23422: done 204 games, reward 156.960, eps 0.01, speed 9.52 f/s, time 40.5 min\n",
            "23518: done 205 games, reward 157.960, eps 0.01, speed 9.19 f/s, time 40.7 min\n",
            "23837: done 206 games, reward 162.360, eps 0.01, speed 8.93 f/s, time 41.3 min\n",
            "23884: done 207 games, reward 160.600, eps 0.01, speed 10.20 f/s, time 41.3 min\n",
            "23929: done 208 games, reward 160.600, eps 0.01, speed 9.79 f/s, time 41.4 min\n",
            "24029: done 209 games, reward 159.720, eps 0.01, speed 9.29 f/s, time 41.6 min\n",
            "24113: done 210 games, reward 157.080, eps 0.01, speed 9.01 f/s, time 41.8 min\n",
            "24172: done 211 games, reward 156.640, eps 0.01, speed 10.27 f/s, time 41.9 min\n",
            "24300: done 212 games, reward 158.840, eps 0.01, speed 9.39 f/s, time 42.1 min\n",
            "24351: done 213 games, reward 157.960, eps 0.01, speed 8.36 f/s, time 42.2 min\n",
            "24531: done 214 games, reward 156.640, eps 0.01, speed 9.57 f/s, time 42.5 min\n",
            "24669: done 215 games, reward 159.280, eps 0.01, speed 9.39 f/s, time 42.7 min\n",
            "24878: done 216 games, reward 158.840, eps 0.01, speed 9.11 f/s, time 43.1 min\n",
            "24922: done 217 games, reward 157.960, eps 0.01, speed 10.03 f/s, time 43.2 min\n",
            "25025: done 218 games, reward 159.280, eps 0.01, speed 9.13 f/s, time 43.4 min\n",
            "25103: done 219 games, reward 159.280, eps 0.01, speed 8.73 f/s, time 43.5 min\n",
            "25181: done 220 games, reward 158.400, eps 0.01, speed 10.10 f/s, time 43.7 min\n",
            "25272: done 221 games, reward 158.400, eps 0.01, speed 8.98 f/s, time 43.8 min\n",
            "25340: done 222 games, reward 155.320, eps 0.01, speed 8.67 f/s, time 44.0 min\n",
            "25399: done 223 games, reward 153.560, eps 0.01, speed 9.96 f/s, time 44.1 min\n",
            "25568: done 224 games, reward 156.200, eps 0.01, speed 9.42 f/s, time 44.4 min\n",
            "25656: done 225 games, reward 154.880, eps 0.01, speed 9.00 f/s, time 44.5 min\n",
            "25720: done 226 games, reward 152.240, eps 0.01, speed 8.65 f/s, time 44.6 min\n",
            "25973: done 227 games, reward 155.760, eps 0.01, speed 9.34 f/s, time 45.1 min\n",
            "26048: done 228 games, reward 153.120, eps 0.01, speed 10.25 f/s, time 45.2 min\n",
            "26115: done 229 games, reward 151.800, eps 0.01, speed 8.73 f/s, time 45.3 min\n",
            "26229: done 230 games, reward 153.120, eps 0.01, speed 9.26 f/s, time 45.6 min\n",
            "26361: done 231 games, reward 153.120, eps 0.01, speed 8.99 f/s, time 45.8 min\n",
            "26426: done 232 games, reward 151.360, eps 0.01, speed 9.20 f/s, time 45.9 min\n",
            "26540: done 233 games, reward 152.240, eps 0.01, speed 9.24 f/s, time 46.1 min\n",
            "26608: done 234 games, reward 151.800, eps 0.01, speed 9.35 f/s, time 46.2 min\n",
            "26727: done 235 games, reward 153.560, eps 0.01, speed 9.57 f/s, time 46.4 min\n",
            "26901: done 236 games, reward 156.200, eps 0.01, speed 9.07 f/s, time 46.8 min\n",
            "27020: done 237 games, reward 152.240, eps 0.01, speed 9.27 f/s, time 47.0 min\n",
            "27081: done 238 games, reward 151.800, eps 0.01, speed 10.16 f/s, time 47.1 min\n",
            "27226: done 239 games, reward 154.360, eps 0.01, speed 9.48 f/s, time 47.3 min\n",
            "27327: done 240 games, reward 152.600, eps 0.01, speed 9.11 f/s, time 47.5 min\n",
            "27446: done 241 games, reward 153.480, eps 0.01, speed 9.26 f/s, time 47.7 min\n",
            "27546: done 242 games, reward 155.240, eps 0.01, speed 9.09 f/s, time 47.9 min\n",
            "27594: done 243 games, reward 153.920, eps 0.01, speed 10.20 f/s, time 48.0 min\n",
            "27642: done 244 games, reward 153.480, eps 0.01, speed 8.17 f/s, time 48.1 min\n",
            "27847: done 245 games, reward 157.440, eps 0.01, speed 9.68 f/s, time 48.4 min\n",
            "27936: done 246 games, reward 156.120, eps 0.01, speed 9.00 f/s, time 48.6 min\n",
            "28024: done 247 games, reward 156.120, eps 0.01, speed 9.07 f/s, time 48.8 min\n",
            "28177: done 248 games, reward 159.640, eps 0.01, speed 9.53 f/s, time 49.0 min\n",
            "28531: done 249 games, reward 160.960, eps 0.01, speed 9.31 f/s, time 49.7 min\n",
            "28593: done 250 games, reward 159.240, eps 0.01, speed 10.29 f/s, time 49.8 min\n",
            "28745: done 251 games, reward 159.680, eps 0.01, speed 9.50 f/s, time 50.0 min\n",
            "28897: done 252 games, reward 156.160, eps 0.01, speed 9.09 f/s, time 50.3 min\n",
            "28948: done 253 games, reward 154.840, eps 0.01, speed 9.29 f/s, time 50.4 min\n",
            "29106: done 254 games, reward 158.800, eps 0.01, speed 9.50 f/s, time 50.7 min\n",
            "29252: done 255 games, reward 159.240, eps 0.01, speed 8.81 f/s, time 51.0 min\n",
            "29327: done 256 games, reward 158.800, eps 0.01, speed 8.83 f/s, time 51.1 min\n",
            "29461: done 257 games, reward 161.000, eps 0.01, speed 9.43 f/s, time 51.3 min\n",
            "29619: done 258 games, reward 157.920, eps 0.01, speed 9.53 f/s, time 51.6 min\n",
            "29678: done 259 games, reward 157.480, eps 0.01, speed 8.51 f/s, time 51.7 min\n",
            "29829: done 260 games, reward 157.920, eps 0.01, speed 9.49 f/s, time 52.0 min\n",
            "29960: done 261 games, reward 156.600, eps 0.01, speed 9.42 f/s, time 52.2 min\n",
            "30078: done 262 games, reward 156.600, eps 0.01, speed 9.31 f/s, time 52.4 min\n",
            "30249: done 263 games, reward 158.800, eps 0.01, speed 9.60 f/s, time 52.7 min\n",
            "30431: done 264 games, reward 157.480, eps 0.01, speed 8.91 f/s, time 53.1 min\n",
            "30616: done 265 games, reward 157.480, eps 0.01, speed 9.50 f/s, time 53.4 min\n",
            "30770: done 266 games, reward 159.240, eps 0.01, speed 9.22 f/s, time 53.7 min\n",
            "30903: done 267 games, reward 160.120, eps 0.01, speed 9.00 f/s, time 53.9 min\n",
            "31025: done 268 games, reward 159.240, eps 0.01, speed 9.20 f/s, time 54.2 min\n",
            "31128: done 269 games, reward 160.120, eps 0.01, speed 9.16 f/s, time 54.3 min\n",
            "31234: done 270 games, reward 160.120, eps 0.01, speed 9.03 f/s, time 54.5 min\n",
            "31284: done 271 games, reward 159.680, eps 0.01, speed 8.60 f/s, time 54.6 min\n",
            "31422: done 272 games, reward 160.120, eps 0.01, speed 8.91 f/s, time 54.9 min\n",
            "31517: done 273 games, reward 158.800, eps 0.01, speed 9.86 f/s, time 55.1 min\n",
            "31578: done 274 games, reward 157.920, eps 0.01, speed 8.53 f/s, time 55.2 min\n",
            "31695: done 275 games, reward 156.600, eps 0.01, speed 9.08 f/s, time 55.4 min\n",
            "31860: done 276 games, reward 157.920, eps 0.01, speed 9.29 f/s, time 55.7 min\n",
            "31908: done 277 games, reward 153.080, eps 0.01, speed 8.46 f/s, time 55.8 min\n",
            "32167: done 278 games, reward 157.480, eps 0.01, speed 8.75 f/s, time 56.3 min\n",
            "32286: done 279 games, reward 157.040, eps 0.01, speed 9.29 f/s, time 56.5 min\n",
            "32350: done 280 games, reward 157.080, eps 0.01, speed 9.91 f/s, time 56.6 min\n",
            "32457: done 281 games, reward 158.840, eps 0.01, speed 9.18 f/s, time 56.8 min\n",
            "32562: done 282 games, reward 155.320, eps 0.01, speed 9.19 f/s, time 57.0 min\n",
            "32627: done 283 games, reward 154.440, eps 0.01, speed 10.22 f/s, time 57.1 min\n",
            "32718: done 284 games, reward 155.760, eps 0.01, speed 8.93 f/s, time 57.3 min\n",
            "32857: done 285 games, reward 155.320, eps 0.01, speed 9.38 f/s, time 57.5 min\n",
            "32948: done 286 games, reward 154.000, eps 0.01, speed 8.96 f/s, time 57.7 min\n",
            "33126: done 287 games, reward 157.080, eps 0.01, speed 9.48 f/s, time 58.0 min\n",
            "33237: done 288 games, reward 156.640, eps 0.01, speed 9.16 f/s, time 58.2 min\n",
            "33288: done 289 games, reward 154.440, eps 0.01, speed 8.79 f/s, time 58.3 min\n",
            "33402: done 290 games, reward 156.640, eps 0.01, speed 9.55 f/s, time 58.5 min\n",
            "33457: done 291 games, reward 155.760, eps 0.01, speed 8.61 f/s, time 58.6 min\n",
            "33505: done 292 games, reward 151.800, eps 0.01, speed 9.99 f/s, time 58.7 min\n",
            "33620: done 293 games, reward 152.240, eps 0.01, speed 9.17 f/s, time 58.9 min\n",
            "33728: done 294 games, reward 154.000, eps 0.01, speed 9.08 f/s, time 59.1 min\n",
            "33803: done 295 games, reward 152.680, eps 0.01, speed 8.80 f/s, time 59.2 min\n",
            "33950: done 296 games, reward 155.320, eps 0.01, speed 9.39 f/s, time 59.5 min\n",
            "33998: done 297 games, reward 152.680, eps 0.01, speed 10.03 f/s, time 59.6 min\n",
            "34124: done 298 games, reward 154.000, eps 0.01, speed 9.29 f/s, time 59.8 min\n",
            "34208: done 299 games, reward 154.440, eps 0.01, speed 8.91 f/s, time 59.9 min\n",
            "34522: done 300 games, reward 157.080, eps 0.01, speed 9.41 f/s, time 60.5 min\n",
            "34616: done 301 games, reward 157.640, eps 0.01, speed 9.00 f/s, time 60.7 min\n",
            "34712: done 302 games, reward 158.520, eps 0.01, speed 9.08 f/s, time 60.8 min\n",
            "34826: done 303 games, reward 158.080, eps 0.01, speed 9.20 f/s, time 61.0 min\n",
            "34984: done 304 games, reward 154.760, eps 0.01, speed 8.83 f/s, time 61.3 min\n",
            "35158: done 305 games, reward 157.720, eps 0.01, speed 9.19 f/s, time 61.7 min\n",
            "35580: done 306 games, reward 156.120, eps 0.01, speed 9.23 f/s, time 62.4 min\n",
            "35656: done 307 games, reward 156.640, eps 0.01, speed 9.73 f/s, time 62.6 min\n",
            "35750: done 308 games, reward 158.400, eps 0.01, speed 9.22 f/s, time 62.7 min\n",
            "35798: done 309 games, reward 156.640, eps 0.01, speed 8.32 f/s, time 62.8 min\n",
            "35983: done 310 games, reward 159.280, eps 0.01, speed 9.48 f/s, time 63.1 min\n",
            "36121: done 311 games, reward 160.160, eps 0.01, speed 9.35 f/s, time 63.4 min\n",
            "36179: done 312 games, reward 158.400, eps 0.01, speed 8.47 f/s, time 63.5 min\n",
            "36237: done 313 games, reward 158.400, eps 0.01, speed 10.06 f/s, time 63.6 min\n",
            "36372: done 314 games, reward 158.400, eps 0.01, speed 9.31 f/s, time 63.8 min\n",
            "36450: done 315 games, reward 156.640, eps 0.01, speed 8.77 f/s, time 64.0 min\n",
            "36575: done 316 games, reward 155.760, eps 0.01, speed 9.25 f/s, time 64.2 min\n",
            "36662: done 317 games, reward 157.520, eps 0.01, speed 9.63 f/s, time 64.4 min\n",
            "36740: done 318 games, reward 155.320, eps 0.01, speed 9.21 f/s, time 64.5 min\n",
            "36796: done 319 games, reward 154.440, eps 0.01, speed 8.84 f/s, time 64.6 min\n",
            "36894: done 320 games, reward 156.640, eps 0.01, speed 9.76 f/s, time 64.8 min\n",
            "36945: done 321 games, reward 155.320, eps 0.01, speed 8.22 f/s, time 64.9 min\n",
            "36997: done 322 games, reward 155.760, eps 0.01, speed 9.95 f/s, time 65.0 min\n",
            "37149: done 323 games, reward 158.400, eps 0.01, speed 9.30 f/s, time 65.2 min\n",
            "37201: done 324 games, reward 155.760, eps 0.01, speed 8.19 f/s, time 65.4 min\n",
            "37332: done 325 games, reward 156.200, eps 0.01, speed 9.15 f/s, time 65.6 min\n",
            "37594: done 326 games, reward 160.160, eps 0.01, speed 9.21 f/s, time 66.1 min\n",
            "37701: done 327 games, reward 155.320, eps 0.01, speed 8.93 f/s, time 66.3 min\n",
            "37786: done 328 games, reward 155.760, eps 0.01, speed 8.14 f/s, time 66.4 min\n",
            "37906: done 329 games, reward 158.400, eps 0.01, speed 9.25 f/s, time 66.7 min\n",
            "37946: done 330 games, reward 156.200, eps 0.01, speed 9.20 f/s, time 66.7 min\n",
            "37994: done 331 games, reward 155.320, eps 0.01, speed 10.03 f/s, time 66.8 min\n",
            "38098: done 332 games, reward 155.760, eps 0.01, speed 8.86 f/s, time 67.0 min\n",
            "38170: done 333 games, reward 154.880, eps 0.01, speed 8.51 f/s, time 67.1 min\n",
            "38220: done 334 games, reward 153.560, eps 0.01, speed 9.83 f/s, time 67.2 min\n",
            "38331: done 335 games, reward 151.800, eps 0.01, speed 8.95 f/s, time 67.4 min\n",
            "38419: done 336 games, reward 149.600, eps 0.01, speed 8.70 f/s, time 67.6 min\n",
            "38585: done 337 games, reward 150.480, eps 0.01, speed 9.22 f/s, time 67.9 min\n",
            "38720: done 338 games, reward 151.800, eps 0.01, speed 9.06 f/s, time 68.2 min\n",
            "38795: done 339 games, reward 149.160, eps 0.01, speed 8.57 f/s, time 68.3 min\n",
            "38964: done 340 games, reward 150.920, eps 0.01, speed 9.12 f/s, time 68.6 min\n",
            "39088: done 341 games, reward 151.800, eps 0.01, speed 8.92 f/s, time 68.8 min\n",
            "39179: done 342 games, reward 151.800, eps 0.01, speed 8.66 f/s, time 69.0 min\n",
            "39304: done 343 games, reward 152.240, eps 0.01, speed 8.94 f/s, time 69.2 min\n",
            "39526: done 344 games, reward 154.880, eps 0.01, speed 8.83 f/s, time 69.7 min\n",
            "39671: done 345 games, reward 150.920, eps 0.01, speed 8.97 f/s, time 69.9 min\n",
            "39773: done 346 games, reward 152.240, eps 0.01, speed 8.75 f/s, time 70.1 min\n",
            "39911: done 347 games, reward 154.440, eps 0.01, speed 9.04 f/s, time 70.4 min\n",
            "40148: done 348 games, reward 154.440, eps 0.01, speed 8.93 f/s, time 70.8 min\n",
            "40255: done 349 games, reward 151.800, eps 0.01, speed 8.87 f/s, time 71.0 min\n",
            "40389: done 350 games, reward 153.080, eps 0.01, speed 9.05 f/s, time 71.3 min\n",
            "40472: done 351 games, reward 152.200, eps 0.01, speed 8.55 f/s, time 71.4 min\n",
            "40574: done 352 games, reward 152.200, eps 0.01, speed 8.11 f/s, time 71.6 min\n",
            "40709: done 353 games, reward 155.280, eps 0.01, speed 8.83 f/s, time 71.9 min\n",
            "40888: done 354 games, reward 153.080, eps 0.01, speed 9.11 f/s, time 72.2 min\n",
            "40956: done 355 games, reward 152.200, eps 0.01, speed 8.36 f/s, time 72.4 min\n",
            "41198: done 356 games, reward 153.080, eps 0.01, speed 8.90 f/s, time 72.8 min\n",
            "41251: done 357 games, reward 151.320, eps 0.01, speed 9.79 f/s, time 72.9 min\n",
            "41339: done 358 games, reward 150.000, eps 0.01, speed 8.61 f/s, time 73.1 min\n",
            "41440: done 359 games, reward 151.320, eps 0.01, speed 8.74 f/s, time 73.3 min\n",
            "41525: done 360 games, reward 150.880, eps 0.01, speed 9.50 f/s, time 73.4 min\n",
            "41599: done 361 games, reward 149.560, eps 0.01, speed 8.68 f/s, time 73.6 min\n",
            "41734: done 362 games, reward 149.560, eps 0.01, speed 8.95 f/s, time 73.8 min\n",
            "41818: done 363 games, reward 148.240, eps 0.01, speed 8.58 f/s, time 74.0 min\n",
            "41879: done 364 games, reward 146.040, eps 0.01, speed 9.68 f/s, time 74.1 min\n",
            "41979: done 365 games, reward 146.480, eps 0.01, speed 8.72 f/s, time 74.3 min\n",
            "42083: done 366 games, reward 145.160, eps 0.01, speed 8.79 f/s, time 74.5 min\n",
            "42169: done 367 games, reward 143.840, eps 0.01, speed 8.54 f/s, time 74.6 min\n",
            "42304: done 368 games, reward 144.280, eps 0.01, speed 8.95 f/s, time 74.9 min\n",
            "42442: done 369 games, reward 146.040, eps 0.01, speed 8.92 f/s, time 75.1 min\n",
            "42510: done 370 games, reward 145.600, eps 0.01, speed 8.32 f/s, time 75.3 min\n",
            "42631: done 371 games, reward 148.240, eps 0.01, speed 8.85 f/s, time 75.5 min\n",
            "42683: done 372 games, reward 146.040, eps 0.01, speed 9.63 f/s, time 75.6 min\n",
            "42834: done 373 games, reward 147.800, eps 0.01, speed 8.90 f/s, time 75.9 min\n",
            "42969: done 374 games, reward 150.000, eps 0.01, speed 8.61 f/s, time 76.1 min\n",
            "43078: done 375 games, reward 151.320, eps 0.01, speed 8.99 f/s, time 76.3 min\n",
            "43203: done 376 games, reward 150.440, eps 0.01, speed 8.86 f/s, time 76.6 min\n",
            "43429: done 377 games, reward 152.640, eps 0.01, speed 8.43 f/s, time 77.0 min\n",
            "43585: done 378 games, reward 149.560, eps 0.01, speed 8.46 f/s, time 77.3 min\n",
            "43676: done 379 games, reward 150.000, eps 0.01, speed 9.48 f/s, time 77.5 min\n",
            "43783: done 380 games, reward 152.160, eps 0.01, speed 8.89 f/s, time 77.7 min\n",
            "43819: done 381 games, reward 150.400, eps 0.01, speed 7.44 f/s, time 77.8 min\n",
            "43866: done 382 games, reward 149.520, eps 0.01, speed 9.63 f/s, time 77.9 min\n",
            "43972: done 383 games, reward 150.840, eps 0.01, speed 8.75 f/s, time 78.1 min\n",
            "44044: done 384 games, reward 149.960, eps 0.01, speed 8.96 f/s, time 78.2 min\n",
            "44109: done 385 games, reward 148.640, eps 0.01, speed 8.94 f/s, time 78.3 min\n",
            "44223: done 386 games, reward 149.960, eps 0.01, speed 8.83 f/s, time 78.5 min\n",
            "44399: done 387 games, reward 148.640, eps 0.01, speed 8.87 f/s, time 78.9 min\n",
            "44450: done 388 games, reward 147.760, eps 0.01, speed 8.35 f/s, time 79.0 min\n",
            "44571: done 389 games, reward 147.320, eps 0.01, speed 8.80 f/s, time 79.2 min\n",
            "44629: done 390 games, reward 145.120, eps 0.01, speed 9.78 f/s, time 79.3 min\n",
            "44774: done 391 games, reward 147.320, eps 0.01, speed 8.49 f/s, time 79.6 min\n",
            "44868: done 392 games, reward 148.200, eps 0.01, speed 9.60 f/s, time 79.7 min\n",
            "45409: done 393 games, reward 153.520, eps 0.01, speed 8.91 f/s, time 80.8 min\n",
            "45504: done 394 games, reward 151.400, eps 0.01, speed 8.91 f/s, time 80.9 min\n",
            "45719: done 395 games, reward 153.600, eps 0.01, speed 9.27 f/s, time 81.3 min\n",
            "45888: done 396 games, reward 152.280, eps 0.01, speed 8.65 f/s, time 81.6 min\n",
            "46010: done 397 games, reward 153.160, eps 0.01, speed 8.97 f/s, time 81.9 min\n",
            "46131: done 398 games, reward 153.600, eps 0.01, speed 8.20 f/s, time 82.1 min\n",
            "46185: done 399 games, reward 151.840, eps 0.01, speed 9.69 f/s, time 82.2 min\n",
            "46232: done 400 games, reward 147.880, eps 0.01, speed 7.89 f/s, time 82.3 min\n",
            "46323: done 401 games, reward 148.200, eps 0.01, speed 9.75 f/s, time 82.5 min\n",
            "46407: done 402 games, reward 149.080, eps 0.01, speed 8.66 f/s, time 82.6 min\n",
            "46508: done 403 games, reward 147.320, eps 0.01, speed 8.76 f/s, time 82.8 min\n",
            "46606: done 404 games, reward 146.000, eps 0.01, speed 8.76 f/s, time 83.0 min\n",
            "46809: done 405 games, reward 145.120, eps 0.01, speed 9.24 f/s, time 83.4 min\n",
            "46897: done 406 games, reward 141.880, eps 0.01, speed 8.61 f/s, time 83.5 min\n",
            "47052: done 407 games, reward 142.240, eps 0.01, speed 9.04 f/s, time 83.8 min\n",
            "47116: done 408 games, reward 141.800, eps 0.01, speed 8.23 f/s, time 84.0 min\n",
            "47298: done 409 games, reward 144.440, eps 0.01, speed 9.02 f/s, time 84.3 min\n",
            "47427: done 410 games, reward 144.440, eps 0.01, speed 8.66 f/s, time 84.5 min\n",
            "47516: done 411 games, reward 143.560, eps 0.01, speed 9.11 f/s, time 84.7 min\n",
            "47703: done 412 games, reward 146.200, eps 0.01, speed 8.70 f/s, time 85.1 min\n",
            "47877: done 413 games, reward 148.400, eps 0.01, speed 9.11 f/s, time 85.4 min\n",
            "47938: done 414 games, reward 146.640, eps 0.01, speed 8.20 f/s, time 85.5 min\n",
            "48101: done 415 games, reward 147.520, eps 0.01, speed 9.04 f/s, time 85.8 min\n",
            "48208: done 416 games, reward 147.080, eps 0.01, speed 8.78 f/s, time 86.0 min\n",
            "48290: done 417 games, reward 146.640, eps 0.01, speed 8.49 f/s, time 86.2 min\n",
            "48605: done 418 games, reward 150.160, eps 0.01, speed 9.05 f/s, time 86.7 min\n",
            "48769: done 419 games, reward 151.480, eps 0.01, speed 8.44 f/s, time 87.1 min\n",
            "48819: done 420 games, reward 150.160, eps 0.01, speed 9.48 f/s, time 87.2 min\n",
            "49021: done 421 games, reward 152.800, eps 0.01, speed 8.12 f/s, time 87.6 min\n",
            "49182: done 422 games, reward 154.120, eps 0.01, speed 8.98 f/s, time 87.9 min\n",
            "49233: done 423 games, reward 151.480, eps 0.01, speed 8.20 f/s, time 88.0 min\n",
            "49395: done 424 games, reward 153.240, eps 0.01, speed 8.75 f/s, time 88.3 min\n",
            "49562: done 425 games, reward 155.880, eps 0.01, speed 8.94 f/s, time 88.6 min\n",
            "49817: done 426 games, reward 154.560, eps 0.01, speed 8.67 f/s, time 89.1 min\n",
            "49955: done 427 games, reward 156.520, eps 0.01, speed 8.47 f/s, time 89.4 min\n",
            "50096: done 428 games, reward 157.400, eps 0.01, speed 8.84 f/s, time 89.6 min\n",
            "50282: done 429 games, reward 157.400, eps 0.01, speed 8.96 f/s, time 90.0 min\n",
            "50456: done 430 games, reward 158.720, eps 0.01, speed 8.53 f/s, time 90.3 min\n",
            "50580: done 431 games, reward 161.360, eps 0.01, speed 8.72 f/s, time 90.5 min\n",
            "50639: done 432 games, reward 160.480, eps 0.01, speed 9.00 f/s, time 90.7 min\n",
            "50743: done 433 games, reward 160.040, eps 0.01, speed 8.78 f/s, time 90.9 min\n",
            "50884: done 434 games, reward 161.360, eps 0.01, speed 8.33 f/s, time 91.1 min\n",
            "50961: done 435 games, reward 161.360, eps 0.01, speed 8.96 f/s, time 91.3 min\n",
            "51164: done 436 games, reward 163.120, eps 0.01, speed 8.54 f/s, time 91.7 min\n",
            "51426: done 437 games, reward 164.440, eps 0.01, speed 8.66 f/s, time 92.2 min\n",
            "51762: done 438 games, reward 165.800, eps 0.01, speed 8.29 f/s, time 92.9 min\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_BeamRider-v5-best_165-20251201-0452-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4.dat\n",
            " - Local:        saved_models/ALE_BeamRider-v5-best_165-20251201-0452-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4.dat\n",
            "Best reward updated 165.000 -> 165.800\n",
            "51808: done 439 games, reward 165.400, eps 0.01, speed 7.73 f/s, time 93.0 min\n",
            "51971: done 440 games, reward 165.840, eps 0.01, speed 8.67 f/s, time 93.3 min\n",
            "52022: done 441 games, reward 163.640, eps 0.01, speed 9.23 f/s, time 93.4 min\n",
            "52444: done 442 games, reward 165.400, eps 0.01, speed 8.50 f/s, time 94.2 min\n",
            "52544: done 443 games, reward 166.720, eps 0.01, speed 8.45 f/s, time 94.4 min\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_BeamRider-v5-best_166-20251201-0453-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4.dat\n",
            " - Local:        saved_models/ALE_BeamRider-v5-best_166-20251201-0453-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4.dat\n",
            "Best reward updated 165.800 -> 166.720\n",
            "52644: done 444 games, reward 166.280, eps 0.01, speed 8.40 f/s, time 94.6 min\n",
            "52790: done 445 games, reward 168.040, eps 0.01, speed 8.71 f/s, time 94.9 min\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_BeamRider-v5-best_168-20251201-0454-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4.dat\n",
            " - Local:        saved_models/ALE_BeamRider-v5-best_168-20251201-0454-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4.dat\n",
            "Best reward updated 166.720 -> 168.040\n",
            "52915: done 446 games, reward 168.040, eps 0.01, speed 8.57 f/s, time 95.1 min\n",
            "53016: done 447 games, reward 166.280, eps 0.01, speed 8.40 f/s, time 95.3 min\n",
            "53197: done 448 games, reward 164.960, eps 0.01, speed 8.46 f/s, time 95.7 min\n",
            "53370: done 449 games, reward 166.280, eps 0.01, speed 8.63 f/s, time 96.0 min\n",
            "53495: done 450 games, reward 167.600, eps 0.01, speed 8.52 f/s, time 96.2 min\n",
            "53542: done 451 games, reward 166.280, eps 0.01, speed 8.09 f/s, time 96.3 min\n",
            "53860: done 452 games, reward 167.600, eps 0.01, speed 8.68 f/s, time 96.9 min\n",
            "54018: done 453 games, reward 166.280, eps 0.01, speed 8.14 f/s, time 97.3 min\n",
            "54068: done 454 games, reward 164.960, eps 0.01, speed 9.22 f/s, time 97.4 min\n",
            "54315: done 455 games, reward 167.600, eps 0.01, speed 8.49 f/s, time 97.8 min\n",
            "54486: done 456 games, reward 168.040, eps 0.01, speed 7.86 f/s, time 98.2 min\n",
            "54533: done 457 games, reward 168.040, eps 0.01, speed 9.26 f/s, time 98.3 min\n",
            "54660: done 458 games, reward 168.480, eps 0.01, speed 8.57 f/s, time 98.5 min\n",
            "54774: done 459 games, reward 168.480, eps 0.01, speed 8.52 f/s, time 98.8 min\n",
            "54915: done 460 games, reward 169.360, eps 0.01, speed 8.17 f/s, time 99.0 min\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_BeamRider-v5-best_169-20251201-0458-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4.dat\n",
            " - Local:        saved_models/ALE_BeamRider-v5-best_169-20251201-0458-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4.dat\n",
            "Best reward updated 168.040 -> 169.360\n",
            "55020: done 461 games, reward 170.240, eps 0.01, speed 8.70 f/s, time 99.2 min\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_BeamRider-v5-best_170-20251201-0458-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4.dat\n",
            " - Local:        saved_models/ALE_BeamRider-v5-best_170-20251201-0458-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4.dat\n",
            "Best reward updated 169.360 -> 170.240\n",
            "55138: done 462 games, reward 169.800, eps 0.01, speed 8.39 f/s, time 99.5 min\n",
            "55227: done 463 games, reward 170.680, eps 0.01, speed 8.78 f/s, time 99.7 min\n",
            "55345: done 464 games, reward 171.560, eps 0.01, speed 8.44 f/s, time 99.9 min\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_BeamRider-v5-best_171-20251201-0459-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4.dat\n",
            " - Local:        saved_models/ALE_BeamRider-v5-best_171-20251201-0459-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4.dat\n",
            "Best reward updated 170.240 -> 171.560\n",
            "55506: done 465 games, reward 171.560, eps 0.01, speed 7.40 f/s, time 100.2 min\n",
            "55557: done 466 games, reward 171.120, eps 0.01, speed 8.79 f/s, time 100.3 min\n",
            "55659: done 467 games, reward 171.560, eps 0.01, speed 8.13 f/s, time 100.6 min\n",
            "55874: done 468 games, reward 173.320, eps 0.01, speed 8.21 f/s, time 101.0 min\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_BeamRider-v5-best_173-20251201-0500-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4.dat\n",
            " - Local:        saved_models/ALE_BeamRider-v5-best_173-20251201-0500-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4.dat\n",
            "Best reward updated 171.560 -> 173.320\n",
            "55942: done 469 games, reward 172.000, eps 0.01, speed 7.79 f/s, time 101.1 min\n",
            "56000: done 470 games, reward 172.880, eps 0.01, speed 8.95 f/s, time 101.2 min\n",
            "56171: done 471 games, reward 172.880, eps 0.01, speed 7.97 f/s, time 101.6 min\n",
            "56401: done 472 games, reward 176.840, eps 0.01, speed 8.13 f/s, time 102.1 min\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_BeamRider-v5-best_176-20251201-0501-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4.dat\n",
            " - Local:        saved_models/ALE_BeamRider-v5-best_176-20251201-0501-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4.dat\n",
            "Best reward updated 173.320 -> 176.840\n",
            "56475: done 473 games, reward 175.600, eps 0.01, speed 7.67 f/s, time 102.2 min\n",
            "56726: done 474 games, reward 179.120, eps 0.01, speed 7.91 f/s, time 102.8 min\n",
            "ðŸ’¾ Model saved to:\n",
            " - Google Drive: /content/drive/MyDrive/PUBLIC/Models/ALE_BeamRider-v5-best_179-20251201-0502-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4.dat\n",
            " - Local:        saved_models/ALE_BeamRider-v5-best_179-20251201-0502-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4.dat\n",
            "Best reward updated 176.840 -> 179.120\n",
            "56816: done 475 games, reward 178.240, eps 0.01, speed 7.76 f/s, time 103.0 min\n",
            "56856: done 476 games, reward 176.480, eps 0.01, speed 8.04 f/s, time 103.0 min\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3856554207.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0mloss_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0mloss_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3979018727.py\u001b[0m in \u001b[0;36mcalc_loss\u001b[0;34m(batch, net, tgt_net, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstates_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_states_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     state_action_values = net(states_t).gather(\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     ).squeeze(-1)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-655458448.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \"\"\"\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             )\n\u001b[0;32m--> 543\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    544\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "257ab3b6"
      },
      "source": [
        "from gymnasium.wrappers import RecordVideo\n",
        "\n",
        "VIDEO_DIR_LOCAL = 'videos'\n",
        "VIDEO_DIR_DRIVE = f\"{save_dir}/videos\"\n",
        "\n",
        "print(f\"VIDEO_DIR_LOCAL: {VIDEO_DIR_LOCAL}\")\n",
        "print(f\"VIDEO_DIR_DRIVE: {VIDEO_DIR_DRIVE}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "247d9179"
      },
      "source": [
        "import os\n",
        "os.makedirs(VIDEO_DIR_LOCAL, exist_ok=True)\n",
        "os.makedirs(VIDEO_DIR_DRIVE, exist_ok=True)\n",
        "\n",
        "print(f\"Created local video directory: {VIDEO_DIR_LOCAL}\")\n",
        "print(f\"Created Google Drive video directory: {VIDEO_DIR_DRIVE}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f271fde9"
      },
      "source": [
        "def save_video_for_model(current_net, device, tag, num_episodes=1):\n",
        "    def make_video_env(env_name, video_dir):\n",
        "        # Create a new environment for video recording\n",
        "        env = gym.make(env_name, render_mode='rgb_array')\n",
        "        env = atari_wrappers.AtariWrapper(env, clip_reward=False, noop_max=0)\n",
        "        env = ImageToPyTorch(env)\n",
        "        env = BufferWrapper(env, n_steps=N_STEPS)\n",
        "        # Wrap with RecordVideo\n",
        "        env = RecordVideo(env, video_folder=video_dir, name_prefix=tag, episode_trigger=lambda x: True)\n",
        "        return env\n",
        "\n",
        "    # Create environments for local and Drive video saving\n",
        "    video_env_local = make_video_env(DEFAULT_ENV_NAME, VIDEO_DIR_LOCAL)\n",
        "    video_env_drive = make_video_env(DEFAULT_ENV_NAME, VIDEO_DIR_DRIVE)\n",
        "\n",
        "    # Load the current network's state dictionary into a new model for evaluation\n",
        "    eval_net = DQN(video_env_local.observation_space.shape, video_env_local.action_space.n).to(device)\n",
        "    eval_net.load_state_dict(current_net.state_dict())\n",
        "    eval_net.eval() # Set to evaluation mode\n",
        "\n",
        "    print(f\"Recording {num_episodes} episodes for tag: {tag}...\")\n",
        "    for i in range(num_episodes):\n",
        "        obs, _ = video_env_local.reset()\n",
        "        video_env_drive.reset() # Reset the drive env too for synchronization in recording logic\n",
        "\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        while not done:\n",
        "            state_v = torch.as_tensor(obs).to(device)\n",
        "            state_v.unsqueeze_(0)\n",
        "            q_vals_v = eval_net(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "\n",
        "            # Step both environments to ensure consistent video recording\n",
        "            obs, reward, is_done, is_tr, _ = video_env_local.step(action)\n",
        "            _, _, _, _, _ = video_env_drive.step(action)\n",
        "\n",
        "            total_reward += reward\n",
        "            done = is_done or is_tr\n",
        "\n",
        "        print(f\"  Episode {i+1} finished with reward: {total_reward}\")\n",
        "\n",
        "    video_env_local.close()\n",
        "    video_env_drive.close()\n",
        "    print(f\"Video recording for tag '{tag}' complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b034d4d7"
      },
      "source": [
        "VIDEO_FPS = 60\n",
        "MIN_VIDEO_DURATION_SECONDS = 10\n",
        "MAX_VIDEO_DURATION_SECONDS = 30\n",
        "VIDEO_RESOLUTION = (640, 480)\n",
        "\n",
        "print(f\"VIDEO_FPS: {VIDEO_FPS}\")\n",
        "print(f\"MIN_VIDEO_DURATION_SECONDS: {MIN_VIDEO_DURATION_SECONDS}\")\n",
        "print(f\"MAX_VIDEO_DURATION_SECONDS: {MAX_VIDEO_DURATION_SECONDS}\")\n",
        "print(f\"VIDEO_RESOLUTION: {VIDEO_RESOLUTION}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a8b9d08"
      },
      "source": [
        "def save_video_for_model(current_net, device, tag, num_episodes=1):\n",
        "    def make_video_env(env_name, video_dir):\n",
        "        # Wrap with RecordVideo first to ensure it captures the full environment output\n",
        "        env = gym.make(env_name, render_mode='rgb_array')\n",
        "        env = RecordVideo(\n",
        "            env,\n",
        "            video_folder=video_dir,\n",
        "            name_prefix=tag,\n",
        "            episode_trigger=lambda x: True,\n",
        "            fps=VIDEO_FPS,\n",
        "            # Apply the desired video resolution\n",
        "            disable_logger=True # disable logger to prevent unnecessary output\n",
        "        )\n",
        "        # Then apply other wrappers like AtariWrapper\n",
        "        env = atari_wrappers.AtariWrapper(env, clip_reward=False, noop_max=0)\n",
        "        env = ImageToPyTorch(env)\n",
        "        env = BufferWrapper(env, n_steps=N_STEPS)\n",
        "        return env\n",
        "\n",
        "    # Calculate max frames based on FPS and max duration\n",
        "    MAX_FRAMES_PER_VIDEO = VIDEO_FPS * MAX_VIDEO_DURATION_SECONDS\n",
        "\n",
        "    # Create environments for local and Drive video saving\n",
        "    video_env_local = make_video_env(DEFAULT_ENV_NAME, VIDEO_DIR_LOCAL)\n",
        "    video_env_drive = make_video_env(DEFAULT_ENV_NAME, VIDEO_DIR_DRIVE)\n",
        "\n",
        "    # Load the current network's state dictionary into a new model for evaluation\n",
        "    eval_net = DQN(video_env_local.observation_space.shape, video_env_local.action_space.n).to(device)\n",
        "    eval_net.load_state_dict(current_net.state_dict())\n",
        "    eval_net.eval() # Set to evaluation mode\n",
        "\n",
        "    print(f\"Recording {num_episodes} episodes for tag: {tag}...\")\n",
        "    for i in range(num_episodes):\n",
        "        obs, _ = video_env_local.reset()\n",
        "        video_env_drive.reset() # Reset the drive env too for synchronization in recording logic\n",
        "\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        frame_counter = 0\n",
        "        while not done and frame_counter < MAX_FRAMES_PER_VIDEO:\n",
        "            state_v = torch.as_tensor(obs).to(device)\n",
        "            state_v.unsqueeze_(0)\n",
        "            q_vals_v = eval_net(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "\n",
        "            # Step both environments to ensure consistent video recording\n",
        "            obs, reward, is_done, is_tr, _ = video_env_local.step(action)\n",
        "            _, _, _, _, _ = video_env_drive.step(action)\n",
        "\n",
        "            total_reward += reward\n",
        "            done = is_done or is_tr\n",
        "            frame_counter += 1\n",
        "\n",
        "        print(f\"  Episode {i+1} finished with reward: {total_reward} (frames: {frame_counter})\")\n",
        "\n",
        "    video_env_local.close()\n",
        "    video_env_drive.close()\n",
        "    print(f\"Video recording for tag '{tag}' complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5ce0585"
      },
      "source": [
        "EARLY_VIDEO_FRAME_THRESHOLD = 5000\n",
        "TRAINED_VIDEO_REWARD_THRESHOLD = 100\n",
        "VIDEO_RECORD_EPISODES = 2\n",
        "\n",
        "print(f\"EARLY_VIDEO_FRAME_THRESHOLD: {EARLY_VIDEO_FRAME_THRESHOLD}\")\n",
        "print(f\"TRAINED_VIDEO_REWARD_THRESHOLD: {TRAINED_VIDEO_REWARD_THRESHOLD}\")\n",
        "print(f\"VIDEO_RECORD_EPISODES: {VIDEO_RECORD_EPISODES}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53fe0f1f"
      },
      "source": [
        "model_comment = f\"lr{LEARNING_RATE}_gamma{GAMMA}_epsdec{EPSILON_DECAY_LAST_FRAME}_rs{REPLAY_START_SIZE}_bs{BATCH_SIZE}_sync{SYNC_TARGET_FRAMES}_fs{N_STEPS}\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "env = make_env(env_name)\n",
        "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "writer = SummaryWriter(comment=f\"-{env_name}-{model_comment}\")\n",
        "print(net)\n",
        "\n",
        "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "agent = Agent(env, buffer)\n",
        "epsilon = EPSILON_START\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "total_rewards = []\n",
        "frame_idx = 0\n",
        "ts_frame = 0\n",
        "ts = time.time()\n",
        "best_m_reward = None\n",
        "\n",
        "hparams = {\n",
        "    'learning_rate': LEARNING_RATE,\n",
        "    'gamma': GAMMA,\n",
        "    'epsilon_start': EPSILON_START,\n",
        "    'epsilon_final': EPSILON_FINAL,\n",
        "    'epsilon_decay_last_frame': EPSILON_DECAY_LAST_FRAME,\n",
        "    'replay_size': REPLAY_SIZE,\n",
        "    'replay_start_size': REPLAY_START_SIZE,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'sync_target_frames': SYNC_TARGET_FRAMES,\n",
        "    'frame_stack': N_STEPS,\n",
        "    'optimizer': 'Adam',\n",
        "    'mean_reward_bound': MEAN_REWARD_BOUND\n",
        "}\n",
        "\n",
        "# Flags to ensure video is recorded only once per trigger point\n",
        "early_video_recorded = False\n",
        "trained_video_recorded = False\n",
        "\n",
        "start_time = time.time()\n",
        "while True:\n",
        "    frame_idx += 1\n",
        "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "    reward = agent.play_step(net, device, epsilon)\n",
        "    if reward is not None:\n",
        "        total_rewards.append(reward)\n",
        "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "        elapsed = time.time() - start_time  # in seconds\n",
        "        ts_frame = frame_idx\n",
        "        ts = time.time()\n",
        "        m_reward = np.mean(total_rewards[-100:])\n",
        "        print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "             f\"eps {epsilon:.2f}, speed {speed:.2f} f/s, time {elapsed/60:.1f} min\")\n",
        "        writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "        writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "        writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
        "        writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "\n",
        "        # --- Video Recording Triggers ---\n",
        "        # 1. Early video recording\n",
        "        if not early_video_recorded and frame_idx >= EARLY_VIDEO_FRAME_THRESHOLD:\n",
        "            print(f\"\\n>>> Triggering early video recording at frame_idx {frame_idx} <<<\")\n",
        "            save_video_for_model(net, device, tag=f\"early_training_f{frame_idx}\", num_episodes=VIDEO_RECORD_EPISODES)\n",
        "            early_video_recorded = True\n",
        "\n",
        "        # 2. When the model starts performing well\n",
        "        if not trained_video_recorded and m_reward >= TRAINED_VIDEO_REWARD_THRESHOLD:\n",
        "            print(f\"\\n>>> Triggering trained video recording (m_reward {m_reward:.3f}) at frame_idx {frame_idx} <<<\")\n",
        "            save_video_for_model(net, device, tag=f\"trained_reward_{int(m_reward)}_f{frame_idx}\", num_episodes=VIDEO_RECORD_EPISODES)\n",
        "            trained_video_recorded = True\n",
        "        # --- End Video Recording Triggers ---\n",
        "\n",
        "        if best_m_reward is None or m_reward > best_m_reward + SAVE_EPSILON:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "            model_filename = f\"{safe_env_name}-best_{int(m_reward)}-{timestamp}-{model_comment}.dat\"\n",
        "\n",
        "            model_path_drive = os.path.join(save_dir_drive, model_filename)\n",
        "            model_path_local = os.path.join(save_dir_local, model_filename)\n",
        "\n",
        "            torch.save(net.state_dict(), model_path_drive)\n",
        "            torch.save(net.state_dict(), model_path_local)\n",
        "\n",
        "            print(f\"ðŸ’¾ Model saved to:\\n - Google Drive: {model_path_drive}\\n - Local:        {model_path_local}\")\n",
        "            if best_m_reward is not None:\n",
        "                print(f\"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}\")\n",
        "            best_m_reward = m_reward\n",
        "\n",
        "            writer.add_hparams(hparams, {'metric/mean_reward': m_reward}, global_step=frame_idx)\n",
        "\n",
        "        if m_reward >= MEAN_REWARD_BOUND:\n",
        "            print(\"Solved in %d frames!\" % frame_idx)\n",
        "            # --- Video Recording Triggers ---\n",
        "            print(f\"\\n>>> Triggering final solved video recording (m_reward {m_reward:.3f}) at frame_idx {frame_idx} <<<\")\n",
        "            save_video_for_model(net, device, tag=f\"solved_reward_{int(m_reward)}_f{frame_idx}\", num_episodes=VIDEO_RECORD_EPISODES)\n",
        "            # --- End Video Recording Triggers ---\n",
        "            break\n",
        "\n",
        "    if len(buffer) < REPLAY_START_SIZE:\n",
        "        continue\n",
        "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "        tgt_net.load_state_dict(net.state_dict())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    batch = buffer.sample(BATCH_SIZE)\n",
        "    loss_t = calc_loss(batch, net, tgt_net, device)\n",
        "    loss_t.backward()\n",
        "    optimizer.step()\n",
        "env.close()\n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cb65f2b"
      },
      "source": [
        "model_comment = f\"lr{LEARNING_RATE}_gamma{GAMMA}_epsdec{EPSILON_DECAY_LAST_FRAME}_rs{REPLAY_START_SIZE}_bs{BATCH_SIZE}_sync{SYNC_TARGET_FRAMES}_fs{N_STEPS}\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "env = make_env(env_name)\n",
        "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "writer = SummaryWriter(comment=f\"-{env_name}-{model_comment}\")\n",
        "print(net)\n",
        "\n",
        "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "agent = Agent(env, buffer)\n",
        "epsilon = EPSILON_START\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "total_rewards = []\n",
        "frame_idx = 0\n",
        "ts_frame = 0\n",
        "ts = time.time()\n",
        "best_m_reward = None\n",
        "\n",
        "hparams = {\n",
        "    'learning_rate': LEARNING_RATE,\n",
        "    'gamma': GAMMA,\n",
        "    'epsilon_start': EPSILON_START,\n",
        "    'epsilon_final': EPSILON_FINAL,\n",
        "    'epsilon_decay_last_frame': EPSILON_DECAY_LAST_FRAME,\n",
        "    'replay_size': REPLAY_SIZE,\n",
        "    'replay_start_size': REPLAY_START_SIZE,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'sync_target_frames': SYNC_TARGET_FRAMES,\n",
        "    'frame_stack': N_STEPS,\n",
        "    'optimizer': 'Adam',\n",
        "    'mean_reward_bound': MEAN_REWARD_BOUND\n",
        "}\n",
        "\n",
        "# Flags to ensure video is recorded only once per trigger point\n",
        "early_video_recorded = False\n",
        "trained_video_recorded = False\n",
        "\n",
        "start_time = time.time()\n",
        "while True:\n",
        "    frame_idx += 1\n",
        "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "    reward = agent.play_step(net, device, epsilon)\n",
        "    if reward is not None:\n",
        "        total_rewards.append(reward)\n",
        "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "        elapsed = time.time() - start_time  # in seconds\n",
        "        ts_frame = frame_idx\n",
        "        ts = time.time()\n",
        "        m_reward = np.mean(total_rewards[-100:])\n",
        "        print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "             f\"eps {epsilon:.2f}, speed {speed:.2f} f/s, time {elapsed/60:.1f} min\")\n",
        "        writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "        writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "        writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
        "        writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "\n",
        "        # --- Video Recording Triggers ---\n",
        "        # 1. Early video recording\n",
        "        if not early_video_recorded and frame_idx >= EARLY_VIDEO_FRAME_THRESHOLD:\n",
        "            print(f\"\\n>>> Triggering early video recording at frame_idx {frame_idx} <<<\")\n",
        "            save_video_for_model(net, device, tag=f\"early_training_f{frame_idx}\", num_episodes=VIDEO_RECORD_EPISODES)\n",
        "            early_video_recorded = True\n",
        "\n",
        "        # 2. When the model starts performing well\n",
        "        if not trained_video_recorded and m_reward >= TRAINED_VIDEO_REWARD_THRESHOLD:\n",
        "            print(f\"\\n>>> Triggering trained video recording (m_reward {m_reward:.3f}) at frame_idx {frame_idx} <<<\")\n",
        "            save_video_for_model(net, device, tag=f\"trained_reward_{int(m_reward)}_f{frame_idx}\", num_episodes=VIDEO_RECORD_EPISODES)\n",
        "            trained_video_recorded = True\n",
        "        # --- End Video Recording Triggers ---\n",
        "\n",
        "        if best_m_reward is None or m_reward > best_m_reward + SAVE_EPSILON:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "            model_filename = f\"{safe_env_name}-best_{int(m_reward)}-{timestamp}-{model_comment}.dat\"\n",
        "\n",
        "            model_path_drive = os.path.join(save_dir_drive, model_filename)\n",
        "            model_path_local = os.path.join(save_dir_local, model_filename)\n",
        "\n",
        "            torch.save(net.state_dict(), model_path_drive)\n",
        "            torch.save(net.state_dict(), model_path_local)\n",
        "\n",
        "            print(f\"\\ud83d\\udcbe Model saved to:\\n - Google Drive: {model_path_drive}\\n - Local:        {model_path_local}\")\n",
        "            if best_m_reward is not None:\n",
        "                print(f\"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}\")\n",
        "            best_m_reward = m_reward\n",
        "\n",
        "            writer.add_hparams(hparams, {'metric/mean_reward': m_reward}, global_step=frame_idx)\n",
        "\n",
        "        if m_reward >= MEAN_REWARD_BOUND:\n",
        "            print(\"Solved in %d frames!\" % frame_idx)\n",
        "            # --- Video Recording Triggers ---\n",
        "            print(f\"\\n>>> Triggering final solved video recording (m_reward {m_reward:.3f}) at frame_idx {frame_idx} <<<\")\n",
        "            save_video_for_model(net, device, tag=f\"solved_reward_{int(m_reward)}_f{frame_idx}\", num_episodes=VIDEO_RECORD_EPISODES)\n",
        "            # --- End Video Recording Triggers ---\n",
        "            break\n",
        "\n",
        "    if len(buffer) < REPLAY_START_SIZE:\n",
        "        continue\n",
        "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "        tgt_net.load_state_dict(net.state_dict())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    batch = buffer.sample(BATCH_SIZE)\n",
        "    loss_t = calc_loss(batch, net, tgt_net, device)\n",
        "    loss_t.backward()\n",
        "    optimizer.step()\n",
        "env.close()\n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74ceda0e"
      },
      "source": [
        "model_comment = f\"lr{LEARNING_RATE}_gamma{GAMMA}_epsdec{EPSILON_DECAY_LAST_FRAME}_rs{REPLAY_START_SIZE}_bs{BATCH_SIZE}_sync{SYNC_TARGET_FRAMES}_fs{N_STEPS}\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "env = make_env(env_name)\n",
        "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "writer = SummaryWriter(comment=f\"-{env_name}-{model_comment}\")\n",
        "print(net)\n",
        "\n",
        "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "agent = Agent(env, buffer)\n",
        "epsilon = EPSILON_START\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "total_rewards = []\n",
        "frame_idx = 0\n",
        "ts_frame = 0\n",
        "ts = time.time()\n",
        "best_m_reward = None\n",
        "\n",
        "hparams = {\n",
        "    'learning_rate': LEARNING_RATE,\n",
        "    'gamma': GAMMA,\n",
        "    'epsilon_start': EPSILON_START,\n",
        "    'epsilon_final': EPSILON_FINAL,\n",
        "    'epsilon_decay_last_frame': EPSILON_DECAY_LAST_FRAME,\n",
        "    'replay_size': REPLAY_SIZE,\n",
        "    'replay_start_size': REPLAY_START_SIZE,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'sync_target_frames': SYNC_TARGET_FRAMES,\n",
        "    'frame_stack': N_STEPS,\n",
        "    'optimizer': 'Adam',\n",
        "    'mean_reward_bound': MEAN_REWARD_BOUND\n",
        "}\n",
        "\n",
        "# Flags to ensure video is recorded only once per trigger point\n",
        "early_video_recorded = False\n",
        "trained_video_recorded = False\n",
        "\n",
        "start_time = time.time()\n",
        "while True:\n",
        "    frame_idx += 1\n",
        "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "    reward = agent.play_step(net, device, epsilon)\n",
        "    if reward is not None:\n",
        "        total_rewards.append(reward)\n",
        "        speed = (frame_idx - ts_frame) / (time.time() - ts)\n",
        "        elapsed = time.time() - start_time  # in seconds\n",
        "        ts_frame = frame_idx\n",
        "        ts = time.time()\n",
        "        m_reward = np.mean(total_rewards[-100:])\n",
        "        print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "             f\"eps {epsilon:.2f}, speed {speed:.2f} f/s, time {elapsed/60:.1f} min\")\n",
        "        writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "        writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "        writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
        "        writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "\n",
        "        # --- Video Recording Triggers ---\n",
        "        # 1. Early video recording\n",
        "        if not early_video_recorded and frame_idx >= EARLY_VIDEO_FRAME_THRESHOLD:\n",
        "            print(f\"\\n>>> Triggering early video recording at frame_idx {frame_idx} <<<\")\n",
        "            save_video_for_model(net, device, tag=f\"early_training_f{frame_idx}\", num_episodes=VIDEO_RECORD_EPISODES)\n",
        "            early_video_recorded = True\n",
        "\n",
        "        # 2. When the model starts performing well\n",
        "        if not trained_video_recorded and m_reward >= TRAINED_VIDEO_REWARD_THRESHOLD:\n",
        "            print(f\"\\n>>> Triggering trained video recording (m_reward {m_reward:.3f}) at frame_idx {frame_idx} <<<\")\n",
        "            save_video_for_model(net, device, tag=f\"trained_reward_{int(m_reward)}_f{frame_idx}\", num_episodes=VIDEO_RECORD_EPISODES)\n",
        "            trained_video_recorded = True\n",
        "        # --- End Video Recording Triggers ---\n",
        "\n",
        "        if best_m_reward is None or m_reward > best_m_reward + SAVE_EPSILON:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "            model_filename = f\"{safe_env_name}-best_{int(m_reward)}-{timestamp}-{model_comment}.dat\"\n",
        "\n",
        "            model_path_drive = os.path.join(save_dir_drive, model_filename)\n",
        "            model_path_local = os.path.join(save_dir_local, model_filename)\n",
        "\n",
        "            torch.save(net.state_dict(), model_path_drive)\n",
        "            torch.save(net.state_dict(), model_path_local)\n",
        "\n",
        "            print(f\"Model saved to:\\n - Google Drive: {model_path_drive}\\n - Local:        {model_path_local}\") # Removed emoji\n",
        "            if best_m_reward is not None:\n",
        "                print(f\"Best reward updated {best_m_reward:.3f} -> {m_reward:.3f}\")\n",
        "            best_m_reward = m_reward\n",
        "\n",
        "            writer.add_hparams(hparams, {'metric/mean_reward': m_reward}, global_step=frame_idx)\n",
        "\n",
        "        if m_reward >= MEAN_REWARD_BOUND:\n",
        "            print(\"Solved in %d frames!\" % frame_idx)\n",
        "            # --- Video Recording Triggers ---\n",
        "            print(f\"\\n>>> Triggering final solved video recording (m_reward {m_reward:.3f}) at frame_idx {frame_idx} <<<\")\n",
        "            save_video_for_model(net, device, tag=f\"solved_reward_{int(m_reward)}_f{frame_idx}\", num_episodes=VIDEO_RECORD_EPISODES)\n",
        "            # --- End Video Recording Triggers ---\n",
        "            break\n",
        "\n",
        "    if len(buffer) < REPLAY_START_SIZE:\n",
        "        continue\n",
        "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "        tgt_net.load_state_dict(net.state_dict())\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    batch = buffer.sample(BATCH_SIZE)\n",
        "    loss_t = calc_loss(batch, net, tgt_net, device)\n",
        "    loss_t.backward()\n",
        "    optimizer.step()\n",
        "env.close()\n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e51ea47d",
        "outputId": "85d0decc-7bf8-4913-8dd1-b1e08c2fc6be"
      },
      "source": [
        "!pip install tensorboard\n",
        "print(\"TensorBoard installed/upgraded successfully.\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.10)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboard) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard) (4.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\n",
            "TensorBoard installed/upgraded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "384fde46",
        "outputId": "b52551e2-bd2d-4cf4-dc3a-3aff2c344ad0"
      },
      "source": [
        "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "print(\"TensorBoard's EventAccumulator imported successfully.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorBoard's EventAccumulator imported successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8902b53f",
        "outputId": "8368aba1-4f77-441e-9aef-b9ab0447fd65"
      },
      "source": [
        "import os\n",
        "\n",
        "log_dir = \"runs\"\n",
        "if not os.path.exists(log_dir):\n",
        "    print(f\"Error: The log directory '{log_dir}' does not exist. Please ensure TensorBoard logs are generated.\")\n",
        "else:\n",
        "    print(f\"Contents of '{log_dir}' directory:\")\n",
        "    for root, dirs, files in os.walk(log_dir):\n",
        "        for name in files:\n",
        "            if \"events.out.tfevents\" in name:\n",
        "                print(os.path.join(root, name))\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of 'runs' directory:\n",
            "runs/Dec01_03-19-11_d9c5e426b41a-ALE/BeamRider-v5-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4/events.out.tfevents.1764559151.d9c5e426b41a.946.0\n",
            "runs/Dec01_03-19-11_d9c5e426b41a-ALE/BeamRider-v5-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4/1764565282.3416421/events.out.tfevents.1764565282.d9c5e426b41a.946.10\n",
            "runs/Dec01_03-19-11_d9c5e426b41a-ALE/BeamRider-v5-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4/1764565323.6956587/events.out.tfevents.1764565323.d9c5e426b41a.946.11\n",
            "runs/Dec01_03-19-11_d9c5e426b41a-ALE/BeamRider-v5-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4/1764564849.4498928/events.out.tfevents.1764564849.d9c5e426b41a.946.5\n",
            "runs/Dec01_03-19-11_d9c5e426b41a-ALE/BeamRider-v5-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4/1764565100.8326662/events.out.tfevents.1764565100.d9c5e426b41a.946.6\n",
            "runs/Dec01_03-19-11_d9c5e426b41a-ALE/BeamRider-v5-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4/1764565112.9031065/events.out.tfevents.1764565112.d9c5e426b41a.946.7\n",
            "runs/Dec01_03-19-11_d9c5e426b41a-ALE/BeamRider-v5-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4/1764564820.7903585/events.out.tfevents.1764564820.d9c5e426b41a.946.4\n",
            "runs/Dec01_03-19-11_d9c5e426b41a-ALE/BeamRider-v5-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4/1764559160.1711152/events.out.tfevents.1764559160.d9c5e426b41a.946.2\n",
            "runs/Dec01_03-19-11_d9c5e426b41a-ALE/BeamRider-v5-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4/1764559158.708322/events.out.tfevents.1764559158.d9c5e426b41a.946.1\n",
            "runs/Dec01_03-19-11_d9c5e426b41a-ALE/BeamRider-v5-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4/1764565151.0749516/events.out.tfevents.1764565151.d9c5e426b41a.946.8\n",
            "runs/Dec01_03-19-11_d9c5e426b41a-ALE/BeamRider-v5-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4/1764564729.035987/events.out.tfevents.1764564729.d9c5e426b41a.946.3\n",
            "runs/Dec01_03-19-11_d9c5e426b41a-ALE/BeamRider-v5-lr0.0001_gamma0.99_epsdec10000_rs1000_bs32_sync500_fs4/1764565217.3638647/events.out.tfevents.1764565217.d9c5e426b41a.946.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "id": "9e5bcf49",
        "outputId": "f1cc560b-5c05-4bae-cefb-27d9f4fa7ba8"
      },
      "source": [
        "import os\n",
        "from glob import glob\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_latest_event_file(log_dir):\n",
        "    list_of_files = glob(os.path.join(log_dir, '**', 'events.out.tfevents.*'), recursive=True)\n",
        "    if not list_of_files:\n",
        "        return None\n",
        "    latest_file = max(list_of_files, key=os.path.getmtime)\n",
        "    return latest_file\n",
        "\n",
        "latest_event_file = get_latest_event_file(log_dir)\n",
        "\n",
        "if latest_event_file:\n",
        "    event_acc = EventAccumulator(latest_event_file)\n",
        "    event_acc.Reload()\n",
        "\n",
        "    # Extract scalar data\n",
        "    rewards = []\n",
        "    losses = []\n",
        "    frames = []\n",
        "    loss_frames = []\n",
        "\n",
        "    if 'reward_100' in event_acc.Tags()['scalars']:\n",
        "        for s in event_acc.Scalars('reward_100'):\n",
        "            rewards.append(s.value)\n",
        "            frames.append(s.step)\n",
        "    else:\n",
        "        print(\"Warning: 'reward_100' not found in scalars. Available tags:\", event_acc.Tags()['scalars'])\n",
        "\n",
        "    if 'loss_t' in event_acc.Tags()['scalars']:\n",
        "        for s in event_acc.Scalars('loss_t'):\n",
        "            losses.append(s.value)\n",
        "            loss_frames.append(s.step)\n",
        "    else:\n",
        "        print(\"Warning: 'loss_t' not found in scalars. Available tags:\", event_acc.Tags()['scalars'])\n",
        "\n",
        "    # Create DataFrames for easier plotting\n",
        "    df_rewards = pd.DataFrame({'frame_idx': frames, 'mean_reward': rewards})\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    if not df_rewards.empty:\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(df_rewards['frame_idx'], df_rewards['mean_reward'], label='Mean Reward (last 100 episodes)', color='blue')\n",
        "        plt.xlabel('Frames')\n",
        "        plt.ylabel('Mean Reward')\n",
        "        plt.title('Mean Reward over Training Frames')\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "    else:\n",
        "        print(\"No reward data to plot.\")\n",
        "\n",
        "    if losses:\n",
        "        df_losses = pd.DataFrame({'frame_idx': loss_frames, 'loss': losses})\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(df_losses['frame_idx'], df_losses['loss'], label='Loss', color='red')\n",
        "        plt.xlabel('Frames')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Loss over Training Frames')\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "    else:\n",
        "        print(\"No loss data to plot.\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Learning curves for mean reward and loss plotted successfully.\")\n",
        "else:\n",
        "    print(\"No TensorBoard event files found in the 'runs' directory.\")\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: 'loss_t' not found in scalars. Available tags: ['epsilon', 'speed', 'reward_100', 'reward']\n",
            "No loss data to plot.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning curves for mean reward and loss plotted successfully.\n"
          ]
        }
      ]
    }
  ]
}
